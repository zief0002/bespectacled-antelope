[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Instructor/TA",
    "section": "",
    "text": "Instructor: Andrew Zieffler  Email: zief0002@umn.edu  Office: Education Sciences Building 178  Office Hours: Tuesday 9:00 AM–10:00 AM; and by appointment  Virtual Office: If you want to meet virtually, send me a Google calendar invite and include a Zoom link in the invite.\n\nTA: Kyle Stagnaro  Email: stagn011@umn.edu  Office: Zoom  Office Hours: Monday 12:00 PM–1:00 PM; and by appointment"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignment Due Dates",
    "section": "",
    "text": "Below are the due dates for the assignments, as well as links to each assignment. The due dates may change at the instructor’s discretion. Any revised due dates will be announced in class and posted to the website.\n\n\n\n\n\n\nAssignment\n\n\nDue Date (T/R)\n\n\nDue Date (M/W)\n\n\nQMD\n\n\nHTML\n\n\n\n\n\n\nAssignment #1: Introduction to Quarto\n\n\n\nFeb. 09\n\n\nFeb. 13\n\n\n\n\n\n\n\n\n\n\nAssignment #2: Polynomial Effects\n\n\n\nFeb. 21\n\n\nFeb. 22\n\n\n\n\n\n\n\n\n\n\nAssignment #3: Evidence and Model Selection\n\n\n\nMar. 02\n\n\nMar. 13\n\n\n\n\n\n\n\n\n\n\nAssignment #4: Logarithmic Transformations\n\n\nMar. 23\n\n\nMar. 27\n\n\n\n\n\n\n\n\n\n\nAssignment #5: Logistic Regression\n\n\n\nApr. 06\n\n\nApr. 10\n\n\n\n\n\n\n\n\n\n\nAssignment #6: LMER: Unconditional Longitudinal Models\n\n\nApr. 20\n\n\nApr. 24\n\n\n\n\n\n\n\n\n\n\nAssignment #7: LMER: More Longitudinal Models\n\n\nMay 06\n\n\nMay 06"
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-quarto.html",
    "href": "assignments/assignment-01-introduction-to-quarto.html",
    "title": "Assignment 01",
    "section": "",
    "text": "The goal of this assignment is to give you experience using Quarto to integrate analysis and documentation. In this assignment, you will use the data from the file fertility.csv to explain variation in infant mortality rates."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-quarto.html#instructions",
    "href": "assignments/assignment-01-introduction-to-quarto.html#instructions",
    "title": "Assignment 01",
    "section": "Instructions",
    "text": "Instructions\nCreate a QMD document to respond to each of the questions below. Please adhere to the following guidelines for further formatting your assignment:\n\nAll graphics should be resized so that they do not take up more room than necessary and should have an appropriate caption. Learn how to do this in a code chunk using knitr syntax.\nAny typed mathematics (equations, matrices, vectors, etc.) should be appropriately typeset within the document using Markdown’s equation typesetting. See here for some examples of how mathematics can be typeset in R Markdown.\nAll syntax should be hidden (i.e., not displayed) unless specifically asked for.\nAny messages or warnings produced (e.g., from loading packages) should also be hidden.\n\nFor each question, specify the question number (e.g., Question 2) using a level-2 (or smaller) header. This assignment is worth 15 points."
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-quarto.html#questions",
    "href": "assignments/assignment-01-introduction-to-quarto.html#questions",
    "title": "Assignment 01",
    "section": "Questions",
    "text": "Questions\n\nImport the data and display the first several rows of data (not all of it). Use one of the paged table options in your YAML to ensure that this is printed nicely. All syntax for these commands should be hidden.\nRecode the gni_class variable by creating a dummy variable called high_gni. To create this dummy variable use the syntax below. Explain in words (not code) how the high_gni dummy variable is being coded.\n\n\nfertility = fertility |>\n  mutate(\n    high_gni = if_else(gni_class == \"Upper/Middle\" | gni_class == \"Upper\", 1, 0)\n  )\n\n\nCreate a well-formatted table that includes the mean infant mortality rate and the standard deviation of infant mortality rates for each of the two High GNI levels represented in the data. Display these in a summary table. All numerical output should be rounded to two decimal places. Also add an appropriate caption (the caption does not have to follow APA formatting). Do not display any syntax.\nUse the lm() function to fit a main effects linear model (unstandardized) regressing infant mortality on your dummy variable and female education level. Use the tidy() function from the broom package to display the model’s coefficient-level output. (Reminder: Do not display any syntax, only the model’s coefficient-level output.)\nUse a bulleted list to provide an interpretation of each estimated regression coefficient (including the intercept) from the regression you fitted in Question 3; one interpretation per list item.\nCreate a well-formatted table of the model’s coefficient tidy() output (e.g., using the gt() function from the {gt} package). In the final outputted table, the five column names should be “Predictor”, “B”, “SE”, “t”, and “p”, respectively. (The last four column names should be italicized since they are statistics.) All numerical output should be rounded to two decimal places, except the p-values, which should be rounded to three decimal places. Also add a caption. (2pts.)\nCreate a publication quality plot that displays the results from the fitted model. For this plot, put the female education level predictor on the x-axis. Display separate lines to show the effects for each level of the dummy-coded GNI variable. The two lines should be displayed using different linetypes or colors (or both) so that they can be easily differentiated in the plot. Be sure that the figure includes a caption using the fig-cap option in the code chunk. The plot should be centered on the page. Adjust the aspect ratio of the plot using fig-width and fig-height in the code chunk so that it looks good. Lastly, change the values of the output width/height (out-width, out-height) to change the size of the plot from the default values. (2pts.)\nUse a display equation to write the equation for the underlying regression model (including error and assumptions) using Greek letters, subscripts, and variable names. Also write the equation for the fitted least squares regression equation based on the output from lm(). Type these two equations in the same display equation, each on a separate line of text in your document, and align the equals signs. (2pts.)\nWrite the following sentence: “The estimated partial regression coefficient (\\(\\hat\\beta_\\mathrm{Female~Education}\\)) is \\(x\\).” In this sentence, use an inline code chunk to replace \\(x\\) with the value for the fitted coefficient from the fitted equation. In this code chunk, do not just write in the value for the coefficient, but use syntax to extract the value from the tidy() output. (Hint: Google “R extract element from dataframe”.) (2pts.)\nCompute the sum of squared residuals for the fitted regression. Although you can use the anova() function to check your work, compute this value by actually using R syntax to carry out the computation, \\(\\sum(y_i - \\hat{y}_i)^2\\). Show the syntax you used in your document.\nWrite a sentence that includes two references in an inline citation. This should also generate the actual references when you knit your document. One of the two references should be the Fox textbook. The other should be a journal article of your choice. You can choose the topic of the sentence and how the two references are used in the citation. (Note the references do not actually have to pertain to what is written in the sentence. This is just an exercise in using the bibliography tools in Markdown.) Specify an appropriate CSL file so that the references and citations are in APA format. (If you want to use a citation style that is different from APA, say for a specific journal, use the appropriate CSL file, and indicate that on the assignment.) Both the BIB and CSL files should be included in your project’s assets directory. (2pts.)"
  },
  {
    "objectID": "assignments/assignment-01-introduction-to-quarto.html#what-to-submit",
    "href": "assignments/assignment-01-introduction-to-quarto.html#what-to-submit",
    "title": "Assignment 01",
    "section": "What to Submit",
    "text": "What to Submit\nYou need to submit a zipped version of your entire assignment-01 project directory. When the TA unzips this and opens your R project file they will render your QMD document. This should produce the HTML file that will be graded. (You can include your HTML file as an extra attachment if you want, but the QMD document will need to render. If it doesn’t render the TA will return it to you to try again.)"
  },
  {
    "objectID": "assignments/assignment-02-polynomial-effects.html",
    "href": "assignments/assignment-02-polynomial-effects.html",
    "title": "Assignment 02",
    "section": "",
    "text": "The goal of this assignment is to give you experience fitting, interpreting, and evaluating models with polynomial effects. In this assignment, you will use the data from the file fertility.csv to explain variation in in infant mortality rates."
  },
  {
    "objectID": "assignments/assignment-02-polynomial-effects.html#instructions",
    "href": "assignments/assignment-02-polynomial-effects.html#instructions",
    "title": "Assignment 02",
    "section": "Instructions",
    "text": "Instructions\nThis assignment needs to be completed using Quarto. Submit a zipped version of your entire assignment-02 project directory. Your QMD document should include a response to each of the questions below. Please adhere to the following guidelines for further formatting your assignment:\n\nAll graphics should be resized so that they do not take up more room than necessary and should have an appropriate caption.\nAny typed mathematics (equations, matrices, vectors, etc.) should be appropriately typeset within the document using Markdown’s equation typesetting.\nAll syntax should be hidden (i.e., not displayed) unless specifically asked for.\nAny messages or warnings produced (e.g., from loading packages) should also be hidden.\n\nFor each question, specify the question number (e.g., Question 2) using a level-2 (or smaller) header. This assignment is worth 15 points."
  },
  {
    "objectID": "assignments/assignment-02-polynomial-effects.html#model-1-linear-effect-of-female-education-level",
    "href": "assignments/assignment-02-polynomial-effects.html#model-1-linear-effect-of-female-education-level",
    "title": "Assignment 02",
    "section": "Model 1: Linear Effect of Female Education Level",
    "text": "Model 1: Linear Effect of Female Education Level\n\nCreate a scatterplot showing the relationship between female education level and infant mortality rates.\nDescribe the relationship between female education level and infant mortality rates. Be sure to comment on the structural form, direction and strength of the relationship. Also comment on any potential observations that deviate from following this relationship (unusual observations or clusters of observations).\nCompute and report the Pearson correlation coefficient between female education level and infant mortality rate. Based on your response to Question 2, explain whether the Pearson correlation coefficient is an appropriate summary measure of the relationship.\nRegress infant mortality rates on female education level. For this model, posit a linear effect of female education level on infant mortality rate (Model 1). Create the scatterplot of the standardized residuals versus the fitted values from Model 1.\nDoes this plot suggest problems about meeting the assumption that the average residual is zero at each fitted value? Explain."
  },
  {
    "objectID": "assignments/assignment-02-polynomial-effects.html#model-2-quadratic-effect-of-female-education-level",
    "href": "assignments/assignment-02-polynomial-effects.html#model-2-quadratic-effect-of-female-education-level",
    "title": "Assignment 02",
    "section": "Model 2: Quadratic Effect of Female Education Level",
    "text": "Model 2: Quadratic Effect of Female Education Level\n\nRegress infant mortality rates on female education level. For this model, posit a quadratic effect of female education level on infant mortality rate (Model 2). Write the fitted equation using Equation Editor (or some other program that correctly types mathematical expressions).\nCompute, report, and interpret the likelihood ratio between Model 2 and Model 1.\nCarry out a likelihood ratio test to compare Model 1 and Model 2. Report the results from this test in a nicely formatted table."
  },
  {
    "objectID": "assignments/assignment-02-polynomial-effects.html#model-3-control-for-differences-in-gross-national-income-gni",
    "href": "assignments/assignment-02-polynomial-effects.html#model-3-control-for-differences-in-gross-national-income-gni",
    "title": "Assignment 02",
    "section": "Model 3: Control for Differences in Gross National Income (GNI)",
    "text": "Model 3: Control for Differences in Gross National Income (GNI)\n\nRegress infant mortality rates on female education level. For this model, posit a quadratic effect of female education level on infant mortality rate, and also control for differences in Gross National Income (Model 3). (Use all four levels of GNI.) Write the fitted equation using Equation Editor (or some other program that correctly types mathematical expressions).\nCarry out a likelihood ratio test to compare Model 2 and Model 3. Add the results from this test to the table you created in Question #8."
  },
  {
    "objectID": "assignments/assignment-02-polynomial-effects.html#adopting-a-model",
    "href": "assignments/assignment-02-polynomial-effects.html#adopting-a-model",
    "title": "Assignment 02",
    "section": "Adopting a Model",
    "text": "Adopting a Model\n\nBased on the results of the two likelihood ratio tests, which model will you adopt? Explain.\nCreate the density plot of the marginal distribution of the standardized residuals for your adopted model, as well as the scatterplot of the standardized residuals versus the fitted values. Place these plots side-by-side in your printed document and, for the purposes of captioning, etc. treat them as two subfigures within a single figure.\nBased on the plots you created in Question 12, evaluate and comment on the tenability of each of the model assumptions."
  },
  {
    "objectID": "assignments/assignment-02-polynomial-effects.html#presenting-the-results",
    "href": "assignments/assignment-02-polynomial-effects.html#presenting-the-results",
    "title": "Assignment 02",
    "section": "Presenting the Results",
    "text": "Presenting the Results\n\nMimic the format and structure of either of the first two tables in the Presenting Results from Many Fitted Regression Models section of the document Creating Tables to Present Statistical Results to create a table to present the numerical information from the three models you fitted in this assignment. Make sure the table you create also has an appropriate caption. If the table is too wide, change the page orientation in your word processing program to “Landscape”, rather than changing the size of the font. (Note: Only this table should be presented in landscape orientation…not your entire assignment!)\nCreate a publication quality plot that displays the fitted curves from Model 3. Display four separate lines to show the effect of Gross National Income. The four lines should be displayed using different linetypes or colors (or both) so that they can be easily differentiated in the plot."
  },
  {
    "objectID": "assignments/assignment-03-evidence-and-model-selection.html",
    "href": "assignments/assignment-03-evidence-and-model-selection.html",
    "title": "Assignment 03",
    "section": "",
    "text": "The goal of this assignment is to build your understanding of using information criteria for model selection. In this assignment, you will use the data from the file wine.csv to examine several different predictors of wine rating (a measure of the wine’s quality). The literature has suggested that price of wine is quite predictive of a wine’s quality."
  },
  {
    "objectID": "assignments/assignment-03-evidence-and-model-selection.html#instructions",
    "href": "assignments/assignment-03-evidence-and-model-selection.html#instructions",
    "title": "Assignment 03",
    "section": "Instructions",
    "text": "Instructions\nSubmit either your QMD and HTML file or, if you are not using Quarto, a PDF file of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll graphics should be resized so that they do not take up more room than necessary and should have an appropriate caption.\nAny typed mathematics (equations, matrices, vectors, etc.) should be appropriately typeset within the document using Markdown’s equation typesetting.\nAll syntax should be hidden (i.e., not displayed) unless specifically asked for.\nAny messages or warnings produced (e.g., from loading packages) should also be hidden.\n\nThis assignment is worth 15 points."
  },
  {
    "objectID": "assignments/assignment-03-evidence-and-model-selection.html#preparation",
    "href": "assignments/assignment-03-evidence-and-model-selection.html#preparation",
    "title": "Assignment 03",
    "section": "Preparation",
    "text": "Preparation\nRead the article: Snipes, M., & Taylor, D. C. (2014). Model selection and Akaike Information Criteria: An example from wine ratings and prices. Wine Economics and Policy, 3(1), 3–9. https://doi.org/10.1016/j.wep.2014.03.001\nYou will be carrying out a form of a robustness study to evaluate the working hypotheses proposed in Snipes and Taylor (2014). In this study, we will fit the same candidate models that Snipes and Taylor fitted in their analysis, however we will be using a different data set (e.g., our data includes more regions than Snipes and Taylor’s data). We will also make one change in the analysis, and that is we will treat vintage as a continuous variable; we won’t categorize it like Snipes and Taylor did. By using a different set of data and making slightly different analytic decisions we can more vigorously evaluate the underlying working hypotheses.\nFit the same nine candidate models that Snipes and Taylor fitted in their analysis, using the wine.csv data. In these models use wine rating (rating) as the outcome. Remember to treat vintage as a continuous variable in the models that include it. These models will be named Model 0, Model 1, …, Model 8 to be consistent with the naming in the Snipes and Taylor paper. Use these fitted models to answer the questions in the assignment."
  },
  {
    "objectID": "assignments/assignment-03-evidence-and-model-selection.html#model-selection-likelihood-framework-of-evidence",
    "href": "assignments/assignment-03-evidence-and-model-selection.html#model-selection-likelihood-framework-of-evidence",
    "title": "Assignment 03",
    "section": "Model Selection: Likelihood Framework of Evidence",
    "text": "Model Selection: Likelihood Framework of Evidence\n\nCompute and report the likelihood for Model 0 given the residuals and set of model assumptions. Use dnorm() for this computation, and show your syntax for full credit.\nCreate a table of the log-likelihoods for the nine candidate models. (Use the logLik() function to compute these values.)\nCompute and interpret the likelihood ratio for comparing the empirical support between Model 2 and Model 3.\nCan we carry out a likelihood ratio test to evaluate whether the amount of empirical support when comparing Model 2 and Model 3 is more than we expect because of sampling error? If so, compute and report the results from the \\(\\chi^2\\)-test. If not, explain why not.\nCompute and interpret the likelihood ratio for comparing the empirical support between Model 2 and Model 5.\nCan we carry out a likelihood ratio test to evaluate whether the amount of empirical support when comparing Model 2 and Model 5 is more than we expect because of sampling error? If so, compute and report the results from the \\(\\chi^2\\)-test. If not, explain why not."
  },
  {
    "objectID": "assignments/assignment-03-evidence-and-model-selection.html#model-selection-information-criteria",
    "href": "assignments/assignment-03-evidence-and-model-selection.html#model-selection-information-criteria",
    "title": "Assignment 03",
    "section": "Model Selection: Information Criteria",
    "text": "Model Selection: Information Criteria\n\nCreate a table of model evidence that includes the following information for each of the nine candidate models. (2pts.)\n\n\nModel\nLog-likelihood\nK\nAICc\n\\(\\Delta\\) AICc\nModel Probability (AICc Weight)\n\nUse this table of model evidence to answer Questions 8–14.\n\nUse the AICc values to select the working hypothesis with the most empirical evidence.\nInterpret the model probability (i.e., AICc weight) for the working hypothesis with the most empirical evidence.\nCompute and interpret the evidence ratio that compares the two working hypotheses with the most empirical evidence.\nBased on previous literature, Snipes and Taylor hypothesized that price was an important predictor of wine quality. Based on your analyses, is price an important predictor of wine quality? Justify your response by referring to the model evidence. (Hint: Pay attention to which models include price and which do not.)\nDoes the empirical evidence support adopting more than one working hypothesis? Justify your response by referring to the model evidence.\nDoes the empirical evidence from the Snipes and Taylor analyses support adopting more than one candidate model? Justify your response by by referring to the model evidence.\nBased on your responses to the last two questions, which set of analyses (yours or Snipes and Taylor) has more model selection uncertainty? Explain."
  },
  {
    "objectID": "assignments/assignment-03-evidence-and-model-selection.html#what-to-submit",
    "href": "assignments/assignment-03-evidence-and-model-selection.html#what-to-submit",
    "title": "Assignment 03",
    "section": "What to Submit",
    "text": "What to Submit\nYou need to submit a zipped version of your entire assignment-01 project directory. When the TA unzips this and opens your R project file they will render your QMD document. This should produce the HTML file that will be graded. (You can include your HTML file as an extra attachment if you want, but the QMD document will need to render. If it doesn’t render the TA will return it to you to try again.)"
  },
  {
    "objectID": "assignments/assignment-04-logarithmic-transformations.html",
    "href": "assignments/assignment-04-logarithmic-transformations.html",
    "title": "Assignment 04",
    "section": "",
    "text": "The goal of this assignment is to give you experience fitting, interpreting, and evaluating models with logarithmically transformed variables. In this assignment, you will use the data from the file wine.csv to examine several different predictors of wine rating (a measure of the wine’s quality). The literature has suggested that price of wine is quite predictive of a wine’s quality. You will be carrying out a replication study (using a different data set) of a study published by Snipes and Taylor (2014)."
  },
  {
    "objectID": "assignments/assignment-04-logarithmic-transformations.html#instructions",
    "href": "assignments/assignment-04-logarithmic-transformations.html#instructions",
    "title": "Assignment 04",
    "section": "Instructions",
    "text": "Instructions\nSubmit either your QMD and HTML file or, if you are not using Quarto, a PDF file of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll graphics should be resized so that they do not take up more room than necessary and should have an appropriate caption.\nAny typed mathematics (equations, matrices, vectors, etc.) should be appropriately typeset within the document using Markdown’s equation typesetting.\nAll syntax should be hidden (i.e., not displayed) unless specifically asked for.\nAny messages or warnings produced (e.g., from loading packages) should also be hidden.\n\nThis assignment is worth 15 points."
  },
  {
    "objectID": "assignments/assignment-04-logarithmic-transformations.html#model-1-effect-of-wine-rating-on-price",
    "href": "assignments/assignment-04-logarithmic-transformations.html#model-1-effect-of-wine-rating-on-price",
    "title": "Assignment 04",
    "section": "Model 1: Effect of Wine Rating on Price",
    "text": "Model 1: Effect of Wine Rating on Price\n\nCreate and examine the scatterplot of the relationship between wine rating (predictor) and price. Include the loess smoother in this plot. Does this plot suggest any nonlinearity in the relationship between wine rating and price that we need to address?\nRegress the log-transformed price variable (using the natural logarithm) on wine rating (Model 1). Report and interpret the slope coefficient (using the log-metric) from the fitted model.\nReport and interpret the back-transformed slope coefficient from Model 1."
  },
  {
    "objectID": "assignments/assignment-04-logarithmic-transformations.html#effect-of-wine-rating-and-region-on-log-transformed-price",
    "href": "assignments/assignment-04-logarithmic-transformations.html#effect-of-wine-rating-and-region-on-log-transformed-price",
    "title": "Assignment 04",
    "section": "Effect of Wine Rating and Region on Log-Transformed Price",
    "text": "Effect of Wine Rating and Region on Log-Transformed Price\nFit two additional models:\n\nA model that includes the effects of whether or not the wine is from California (i.e., california) to predict variation in the log-transformed price (Model 2).\nA model that includes the effects of wine rating and whether or not the wine is from California (i.e., california) to predict variation in the log-transformed price (Model 3).\n\n\nInterpret the effect associated with california predictor (using the log-metric) from Model 3.\nReport and interpret the back-transformed coefficient associated with california predictor from Model 3.\n\nFit a model that includes both the the wine rating and california main effects, as well as, the interaction effect between those predictors to predict variation in the log-transformed price (Model 4).\n\nCreate a table to present the numerical information from the three models you fitted in this assignment along with the AICc values. (Mimic the Presenting Results from Many Fitted Regression Models section of the document Creating Tables to Present Statistical Results to create this table. Include the AICc value below the RMSE value in the table.) Make sure the table you create also has an appropriate caption. If the table is too wide, change the page orientation in your word processing program to “Landscape”, rather than changing the size of the font. (Note: Only this table should be presented in landscape orientation…not your entire assignment!) (3pts.)"
  },
  {
    "objectID": "assignments/assignment-04-logarithmic-transformations.html#adopting-a-final-candidate-model",
    "href": "assignments/assignment-04-logarithmic-transformations.html#adopting-a-final-candidate-model",
    "title": "Assignment 04",
    "section": "Adopting a “Final” Candidate Model",
    "text": "Adopting a “Final” Candidate Model\n\nBased on the model evidence, which of the candidate models will you adopt as your “final” model? Explain.\nWrite the fitted equation for the adopted candidate model.\nCreate and report a set of residual plots that allow you to evaluate the adopted model’s assumptions. Are the assumptions for the model satisfied? Explain. (2pts.)"
  },
  {
    "objectID": "assignments/assignment-04-logarithmic-transformations.html#presenting-the-results",
    "href": "assignments/assignment-04-logarithmic-transformations.html#presenting-the-results",
    "title": "Assignment 04",
    "section": "Presenting the Results",
    "text": "Presenting the Results\n\nCreate a publication quality plot that displays the fitted curve(s) from your adopted candidate model. If you show more than one curve, each line should be easily differentiated in the plot. (Note: Make sure that you back-transform any log-transformed variables when you create this plot.) (2pts.)\nUse the plot to help describe/interpret the effect of wine rating on price."
  },
  {
    "objectID": "assignments/assignment-05-logistic-regression.html",
    "href": "assignments/assignment-05-logistic-regression.html",
    "title": "Assignment 05",
    "section": "",
    "text": "This goal of this assignment is to give you experience working with working with logistic regression models to analyze dichotomous outcome data. In this assignment, you will use the data from the file same-sex-marriage.csv to examine the effects of two aspects of religion (denomination, and frequency of attendance of religious services) on the support of same-sex marriage."
  },
  {
    "objectID": "assignments/assignment-05-logistic-regression.html#instructions",
    "href": "assignments/assignment-05-logistic-regression.html#instructions",
    "title": "Assignment 05",
    "section": "Instructions",
    "text": "Instructions\nSubmit either your QMD and HTML file or, if you are not using Quarto, a PDF file of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll graphics should be resized so that they do not take up more room than necessary and should have an appropriate caption.\nAny typed mathematics (equations, matrices, vectors, etc.) should be appropriately typeset within the document using Markdown’s equation typesetting.\nAll syntax should be hidden (i.e., not displayed) unless specifically asked for.\nAny messages or warnings produced (e.g., from loading packages) should also be hidden.\n\nThis assignment is worth 15 points."
  },
  {
    "objectID": "assignments/assignment-05-logistic-regression.html#effect-of-religious-service-attendance",
    "href": "assignments/assignment-05-logistic-regression.html#effect-of-religious-service-attendance",
    "title": "Assignment 05",
    "section": "Effect of Religious Service Attendance",
    "text": "Effect of Religious Service Attendance\nYou will begin the analysis by examining the effect of religious service attendance on support of same-sex marriage. Because the data for this predictor come from a Likert scale (ordinal in nature), we need to examine whether we can treat it as a continuous predictor in the model, or whether we should treat it as categorical.\n\nBegin by computing the empirical proportion of people that support same-sex marriage for each of the attendance categories. Create a line plot that shows the relationship between proportion of support and attendance.\n\nBased on the plot you just created, the relationship between proportion of support and attendance seems linear. Because of this, we can treat the Likert data as continuous; using a line (or polynomial) to fit the relationship. The only caution being that when interpreting a slope, we say something like, “a one-unit difference in \\(X\\) is associated with a \\(\\hat\\beta_1\\)-unit difference in \\(Y\\)”. For ordinal (Likert) data a one-unit difference in \\(X\\) really indicates a shift from one category to the next highest category.\n\nFit two logistic models to the data using religious service attendance. In the first model, only include the linear effect of attendance. In the second model, include both a linear and quadratic effect to predict variation in support for same-sex marriage. Which model should be adopted (linear or quadratic)? Justify your response by providing any statistical evidence you used in reaching your decision. (Note: The model you adopt here will be henceforth referred to as Model 1.)\n\n\n\n\nWrite the fitted equation for Model 1. (Don’t forget to include all appropriate subscripts. Also define any terms in the model that are ambiguous.)\nUse the fitted equation for Model 1 to predict the (a) log-odds, (b) odds, and (c) probability of someone supporting same-sex marriage if that person attends religious services almost every week."
  },
  {
    "objectID": "assignments/assignment-05-logistic-regression.html#effect-of-denomination",
    "href": "assignments/assignment-05-logistic-regression.html#effect-of-denomination",
    "title": "Assignment 05",
    "section": "Effect of Denomination",
    "text": "Effect of Denomination\n\nAre there sample differences in the proportion of people who support same-sex marriage between the denominations? Explain by using evidence from the sample data.\n\nFit a logistic model to the data using denomination to predict variation in support for same-sex marriage (Model 2). In this model, use Protestant as the reference group.\n\nInterpret the effect associated with the Jewish coefficient in terms of (a) log-odds, and (b) odds."
  },
  {
    "objectID": "assignments/assignment-05-logistic-regression.html#effect-of-attendance-and-denomination",
    "href": "assignments/assignment-05-logistic-regression.html#effect-of-attendance-and-denomination",
    "title": "Assignment 05",
    "section": "Effect of Attendance and Denomination",
    "text": "Effect of Attendance and Denomination\nFit the logistic model that includes all the adopted effects for religous service attendance and the effects of denomination to predict variation in support of same-sex marriage. (Note: This model will be referred to as Model 3.)\n\nCompute and report one of the pseudo-\\(R^2\\) values for Model 3. Also provide an interpretation of this measure."
  },
  {
    "objectID": "assignments/assignment-05-logistic-regression.html#adding-covariates",
    "href": "assignments/assignment-05-logistic-regression.html#adding-covariates",
    "title": "Assignment 05",
    "section": "Adding Covariates",
    "text": "Adding Covariates\nNow you will examine three potential covariates (friends, age, and female) that have been linked in the substantive literature to support of same-sex marriage.\n\nTo help think about which covariates should be included in your model, create a correlation matrix of the outcome, and all three covariates. By referring to this matrix, clearly indicate the order in which you will add covariates into the model.\n\nFit three logistic models based on the order of importance of the three covariates that also include the effects of religious attendance, and denomination. For example, the first of these three models would include the effects of religious attendance, denomination, and the most important covariate you identifed in Question 8. The second model would include the effects of religious attendance, denomination, and the two most important covariates you identified. Finally, the third model would include the effects of religious attendance, denomination, and all three covariates.\n\nWhich of these three models should be adopted as you final model (or set of models)? Justify your response by providing any statistical evidence you used in reaching your decision. (2pts.)\nCreate a table of results from your set of fitted models. This table should include Models 1–3 and also any model(s) you adopted from the previous question. Like other regression tables you have created, be sure to include the estimated coefficients and standard errors for each of the effects included in the models. (To be consistent with ASA recommendations, do not include p-values or stars.) Also include the AICc values for each model. (2pts.)"
  },
  {
    "objectID": "assignments/assignment-05-logistic-regression.html#interpreting-the-final-adopted-model",
    "href": "assignments/assignment-05-logistic-regression.html#interpreting-the-final-adopted-model",
    "title": "Assignment 05",
    "section": "Interpreting the Final Adopted Model",
    "text": "Interpreting the Final Adopted Model\n\nCreate a plot that visually displays the results of your final adopted fitted model. Be sure to visually show the effects the focal predictors (religious service attendance and denomination). Also show any pertinent covariates you think are necessary to include. (Think about how the inclusion of the covariates help readers better understand the effects of the focal predictors.) (2pts.)\nWrite a few sentences that tell the data narrative about the effects of religious service attendance and denomination on the support of same-sex marriage. Use the models in your table of model results to help create this narrative. Keep the focus on the focal variables in this narrative."
  },
  {
    "objectID": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html",
    "href": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html",
    "title": "Assignment 06",
    "section": "",
    "text": "This goal of this assignment is to give you experience working with mixed-effects regression models to analyze longitudinal data. In this assignment, you will use the data from the file nhl.csv to examine longitudinal variation in cost of attending an NHL game."
  },
  {
    "objectID": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#instructions",
    "href": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#instructions",
    "title": "Assignment 06",
    "section": "Instructions",
    "text": "Instructions\nSubmit either your QMD and HTML file or, if you are not using Quarto, a PDF file of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll graphics should be resized so that they do not take up more room than necessary and should have an appropriate caption.\nAny typed mathematics (equations, matrices, vectors, etc.) should be appropriately typeset within the document using Markdown’s equation typesetting.\nAll syntax should be hidden (i.e., not displayed) unless specifically asked for.\nAny messages or warnings produced (e.g., from loading packages) should also be hidden.\n\nThis assignment is worth 13 points."
  },
  {
    "objectID": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#preparation",
    "href": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#preparation",
    "title": "Assignment 06",
    "section": "Preparation",
    "text": "Preparation\nAfter importing the data set, create a new variable called c_year that centers the year values at 2002. To center a variable we subtract the value we want to center at. (e.g., \\(\\mathtt{c\\_year}=\\mathtt{year}-2002\\)). This variable will represent the number of years since 2002.\nFor all analyses in this assignment, unless otherwise requested, use the c_year variable and not the year variable."
  },
  {
    "objectID": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#data-exploration",
    "href": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#data-exploration",
    "title": "Assignment 06",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nCreate and report a plot to display the cost of attending an NHL game (FCI) over time for each team (team profiles). In this plot, all teams should be in the same panel. Also add the profile based on the mean cost of attending an NHL game FCI over time. Make the teams’ profiles slightly transparent so that the mean profile is easily visible.\nBased on the average growth profile, describe how the average cost of attending an NHL game has changed over time. Your description of the growth pattern should allude to the functional form for the model."
  },
  {
    "objectID": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#fitting-and-evaluating-models",
    "href": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#fitting-and-evaluating-models",
    "title": "Assignment 06",
    "section": "Fitting and Evaluating Models",
    "text": "Fitting and Evaluating Models\nFit the following three models. For each of the growth models, include only a random-effect of intercept in the model. Use maximum likelihood estimation to fit all the models.\n\nModel A: Unconditional random intercepts model\nModel B: Unconditional linear growth model\nModel C: Unconditional quadratic growth model\n\n\nCreate and report a table that includes the estimated variance components for each of the three fitted models.\nUse the estimated variance components from Model A to determine the proportion of variation unaccounted for at the between- and within-team levels. Report these values.\nUse the estimated variance components to determine the variation accounted for at the between- and within-team levels based on Model B. Report these values.\nWhich source of variation did Model B most account for? Explain why you would expect this based on the predictor included in Model B."
  },
  {
    "objectID": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#adopting-and-evaluating-an-unconditional-growth-model",
    "href": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#adopting-and-evaluating-an-unconditional-growth-model",
    "title": "Assignment 06",
    "section": "Adopting and Evaluating an Unconditional Growth Model",
    "text": "Adopting and Evaluating an Unconditional Growth Model\n\nBased on the AICc, which model (A, B, or C) has the most empirical evidence? Explain.\nCreate and report a density plot of the residuals and a scatterplot of the residuals versus the fitted values for the model you identified in Question 8. You can use the augment() function from {broom.mixed} to obtain both the fitted values (.fitted) and residuals (.resid). We evaluate these residual plots using the same methods we do when we fit an lm() model, so add any confidence envelopes, smoothers, or other guides to these plots that will aid in your evaluation.\nEvaluate the assumptions of normality, homogeneity of variance, and that the average residual is equal to zero based on the plots you created in Question 9."
  },
  {
    "objectID": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#reporting-the-results-from-the-adopted-model",
    "href": "assignments/assignment-06-lmer-unconditional-longitudinal-models.html#reporting-the-results-from-the-adopted-model",
    "title": "Assignment 06",
    "section": "Reporting the Results from the Adopted Model",
    "text": "Reporting the Results from the Adopted Model\n\nWrite the global fitted equation for the model you adopted in Question 7.\nWrite the team-specific fitted equation for the Minnesota Wild based on the model you adopted in Question 7.\nCreate a plot showing the predicted cost of attending an NHL game over time based on the global fitted model you reported in Question 10. Be sure to include a caption.\nAdd a line to the plot you created in Question #12 (don’t re-create the plot) showing the predicted cost of attending a Minnesota Wild game. Update the caption by adding an additional sentence to differentiate the Minnesota Wild curve from the global curve."
  },
  {
    "objectID": "assignments/assignment-07-longitudinal-analysis-ii.html",
    "href": "assignments/assignment-07-longitudinal-analysis-ii.html",
    "title": "Assignment 07",
    "section": "",
    "text": "This goal of this assignment is to give you more experience working with mixed-effects regression models to analyze longitudinal data. In this assignment, you will again use the data from the file nhl.csv to examine longitudinal variation in cost of attending an NHL game."
  },
  {
    "objectID": "assignments/assignment-07-longitudinal-analysis-ii.html#instructions",
    "href": "assignments/assignment-07-longitudinal-analysis-ii.html#instructions",
    "title": "Assignment 07",
    "section": "Instructions",
    "text": "Instructions\nSubmit either your QMD and HTML file or, if you are not using Quarto, a PDF file of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\n\nAll graphics should be resized so that they do not take up more room than necessary and should have an appropriate caption.\nAny typed mathematics (equations, matrices, vectors, etc.) should be appropriately typeset within the document using Markdown’s equation typesetting.\nAll syntax should be hidden (i.e., not displayed) unless specifically asked for.\nAny messages or warnings produced (e.g., from loading packages) should also be hidden.\n\nThis assignment is worth 12 points."
  },
  {
    "objectID": "assignments/assignment-07-longitudinal-analysis-ii.html#preparation",
    "href": "assignments/assignment-07-longitudinal-analysis-ii.html#preparation",
    "title": "Assignment 07",
    "section": "Preparation",
    "text": "Preparation\nAfter importing the data set, create a new variable called c_year that centers the year values at 2002. This variable will represent the number of years since 2002. For all analyses in this assignment, unless otherwise requested, use the c_year variable and not the year variable."
  },
  {
    "objectID": "assignments/assignment-07-longitudinal-analysis-ii.html#data-exploration",
    "href": "assignments/assignment-07-longitudinal-analysis-ii.html#data-exploration",
    "title": "Assignment 07",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nCreate and report a plot to display the cost of attending an NHL game (FCI) over time facetted on whether or not high school hockey is a tradition in the team’s location. This plot should include the average and team profiles for each facet. Make the teams’ profiles slightly transparent so that the mean profile is easily visible.\nDo the teams’ profiles indicate that a random-effect of linear growth should be included in the candidate models? Explain.\nBased on the plot created in Question #1, indicate whether models that include an effect of high school hockey tradition should be included in the candidate set of models. If this effect should be included, does the data suggest a main-effect or an interaction-effect with time? Explain. (2pts)"
  },
  {
    "objectID": "assignments/assignment-07-longitudinal-analysis-ii.html#fitting-and-evaluating-models",
    "href": "assignments/assignment-07-longitudinal-analysis-ii.html#fitting-and-evaluating-models",
    "title": "Assignment 07",
    "section": "Fitting and Evaluating Models",
    "text": "Fitting and Evaluating Models\nFit the following five models:\n\nModel A: Unconditional random intercepts model\nModel B: Unconditional growth model (with only intercept random-effect)\nModel C: Unconditional growth model (with both intercept and linear growth random-effects)\nModel D: Conditional growth model with main-effect of high school hockey (with both intercept and linear growth random-effects)\nModel E: Conditional growth model with interaction-effect between high school hockey and time (with both intercept and linear growth random-effects)\n\nUse maximum likelihood estimation to fit all the models.\n\nCreate and report a table of regression results. In this table Include the following information for each of the five candidate models (5pts):\n\nEstimated fixed-effect coefficients and standard errors;\nEstimated variance components;\nPseudo \\(R^2\\) measures based on the reduction in the variance components;\nModel-level deviance and AICc measures; and\nAny notes necessary to help the reader"
  },
  {
    "objectID": "assignments/assignment-07-longitudinal-analysis-ii.html#most-empirically-supported-model",
    "href": "assignments/assignment-07-longitudinal-analysis-ii.html#most-empirically-supported-model",
    "title": "Assignment 07",
    "section": "Most Empirically Supported Model",
    "text": "Most Empirically Supported Model\nBased on the AICc values use the model with the most empirical evidence to answer the remaining questions.\n\nWrite the multilevel equations for the statistical model (not the fitted equations) for the model with the most empirical evidence. Don’t forget to include the assumptions!\nWrite the team-specific fitted equation for the Minnesota Wild based on the model that has the most empirical evidence.\nUse the results from the fitted model with the most empirical evidence to create a plot that displays the predicted average cost of attending an NHL game as a function of time for teams that have a tradition of high school and those that do not.\nAdd a line to the plot you created in Question #7 (don’t re-create the plot) showing the predicted cost of attending a Minnesota Wild game.\nWrite a caption for your plot (3–4 sentences) that help a reader understand the effect of time and high school hockey tradition on cost of attending an NHL game and also how the cost of attending a Wild game compares to the average team over time. (2pts)"
  },
  {
    "objectID": "codebooks/woods.html",
    "href": "codebooks/woods.html",
    "title": "woods.csv",
    "section": "",
    "text": "The data in woods.csv were collected from several sources to assesses the effect of political corruption on state environmental policy (Woods, 2008)."
  },
  {
    "objectID": "codebooks/woods.html#attributes",
    "href": "codebooks/woods.html#attributes",
    "title": "woods.csv",
    "section": "Attributes",
    "text": "Attributes\n\nstate: Two-letter state postal code\nenv_prog_str: Environmental program strength is measured via Hall and Kerr’s (1991) Green Policy Index, a composite score that represents 67 state policy initiatives in a variety of environmental arenas, including air, water, and hazardous waste. The index includes indicators such as the sanctions available to the appropriate agencies in each state, the size of the state’s pollution monitoring program, and the size of the state’s program budget, as well as a variety of specific policy indicators.\ncorrupt: Aggregate number of convictions (per 100 officials) during the Reagan Administration (between 1981 and 1987). Aggregating across several years helps eliminate spikes that occur because a single investigation may result in multiple convictions. The average state had 1.42 convictions per 100 officials over that period.\nwealth: Gross state product per capita, in thousands of dollars. This is an indicator of a state’s financial resources.\ntoxic_waste: Natural logarithm of the total tons of toxic waste emitted in air, water based on the U.S. EPA’s Toxic Release Inventory. Higher values indicate a more severe problem.\ndem_party_control: Represents the average amount of Democratic Party control of state political institutions in the 1980s measured as a proportion.\ninterparty_comp: Holbrook and Van Dunk’s (1993) district-level measure is included as an indicator of interparty competition.\npublic_env: Indication of public attitudes toward environmental protection in the state. It is calculated from the 1988–1992 NES Senate Election Study, which asked the following question: “Should federal spending on the environment be increased, decreased, or stay the same?” Individual responses are coded 1 (decrease), 2 (same), and 3 (increase) and then averaged.\nmanuf_groups: The number of manufacturing groups registered to lobby in the state in 1990 standardized by gross state product to account for the fact that the density of organized interests is greater in states with larger economies.\nenv_groups: The number of environmental groups registered to lobby in the state in 1990 standardized by gross state product to account for the fact that the density of organized interests is greater in states with larger economies."
  },
  {
    "objectID": "codebooks/woods.html#preview",
    "href": "codebooks/woods.html#preview",
    "title": "woods.csv",
    "section": "Preview",
    "text": "Preview\n\n# Import data\nwoods = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/woods.csv\")\n\n# View data\nwoods |>\n    print(width = Inf)\n\n# A tibble: 50 × 10\n   state env_prog_str corrupt wealth toxic_waste dem_party_control\n   <chr>        <dbl>   <dbl>  <dbl>       <dbl>             <dbl>\n 1 AK               8   0.586   44.3        1.31             0.625\n 2 AL               6   0.630   18.9        4.70             0.800\n 3 AR               5   0.132   18.4        3.71             0.900\n 4 AZ              13   0.318   19.3        4.19             0.300\n 5 CA              34   0.498   25.5        4.69             0.600\n 6 CO              17   0.212   23.8        2.75             0.5  \n 7 CT              29   0.213   30.2        3.63             0.900\n 8 DE              15   0.301   34.3        2.15             0.300\n 9 FL              24   0.604   19.9        4.12             0.800\n10 GA              12   0.546   22.7        4.46             1    \n   interparty_comp public_env manuf_groups env_groups\n             <dbl>      <dbl>        <dbl>      <dbl>\n 1           53.5        2.42     0.000654  0.000346 \n 2           27.3        2.37     0.000538  0.000064 \n 3            9.26       2.42     0.000636  0.0000455\n 4           33.9        2.52     0.000783  0.000148 \n 5           47.3        2.7      0.000259  0.000014 \n 6           40.2        2.5      0.000521  0.000242 \n 7           52.8        2.59     0.000586  0.000192 \n 8           39.7        2.60     0.00105   0.000127 \n 9           31.1        2.61     0.000875  0.000246 \n10           16.2        2.49     0.000651  0.000143 \n# … with 40 more rows"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Name\n\n\nData\n\n\nCodebook\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarbon.csv\n\n\n  \n\n\n  \n\n\n\n\nfertility.csv\n\n\n  \n\n\n  \n\n\n\n\ngraduation.csv\n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmammals.csv\n\n\n\n\n\n\n\n\n\n\nminneapolis.csv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmn-schools.csv\n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnhl.csv\n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nriverview.csv\n\n\n  \n\n\n  \n\n\n\n\nsame-sex-marriage.csv\n\n\n  \n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvocabulary.csv\n\n\n  \n\n\n  \n\n\n\n\nwine.csv\n\n\n  \n\n\n  \n\n\n\n\nwoods.csv"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EPsy 8252",
    "section": "",
    "text": "In this class, we will work together to develop a learning community that is inclusive and respectful, and where every student is supported in the learning process. As a class full of diverse individuals (reflected by differences in race, culture, age, religion, gender identity, sexual orientation, socioeconomic background, abilities, professional goals, and other social identities and life experiences) I expect that different students may need different things to support and promote their learning. The TAs and I will do everything we can to help with this, but as we only know what we know, we need you to communicate with us if things are not working for you or you need something we are not providing. I hope you all feel comfortable in helping to promote an inclusive classroom through respecting one another’s individual differences, speaking up, and challenging oppressive/problematic ideas. Finally, I look forward to learning from each of you and the experiences you bring to the class.\n\n\n\n\n\n\nTuesday/Thursday (11:15am–12:30pm): Appleby Hall 302\nMonday/Wednesday (2:30pm–3:45pm): Peik Hall 225\n\n\n\n\n\n\nThe course syllabus is available here.\nMessage from Snoop Dogg about the syllabus\n\n\n\n\n\nThe course textbooks are available via the University of Minnesota library.\n\nRequired: Fox, J. (2013). A mathematical primer for social statistics. Sage.\nOptional: Anderson, D. R. (2008). Model based inference in the life sciences: A primer on evidence. Springer.\n\n\n\n\n\n\nStatistical Modeling and Computation for Educational Scientists\n\nLearn about simple and multiple regression\nLearn about Ordinary Least squares (OLS)\nLearn about correlation/standardized regression\nLearn about regrression assumptions\nLearn about dummy coding for dichotomous/polychotomous predictors\nLearn about interaction effects\n\nComputational Toolkit for Educational Scientists\n\nLearn basics of R\nLearn {dplyr} for wrangling data\nLearn {ggplot2} for visualizing data"
  },
  {
    "objectID": "notes/02-project-organization.html",
    "href": "notes/02-project-organization.html",
    "title": "Project Organization",
    "section": "",
    "text": "In this set of notes, you will learn some basic tips and tricks for organizing directories and files for project management. At the end of it, you will have an organized project directory to begin work on Assignment 1."
  },
  {
    "objectID": "notes/02-project-organization.html#naming-conventions",
    "href": "notes/02-project-organization.html#naming-conventions",
    "title": "Project Organization",
    "section": "Naming Conventions",
    "text": "Naming Conventions\nThe naming conventions for the directories and files in our project are as follows:\n\nFile names should be short but descriptive (less than 25 characters)\nAll lowercase letters\nAvoid special characters and spaces in a file name\n\nUse hyphens instead of spaces to separate words (e.g., assignment-01)\n\nAny names that include the date will use the ISO 8601 date format (YYYYMMDD)\nAny names that include a number will include at least two digits (e.g., assignment-01 rather than assignment-1)\n\nAgain, while there is no one best naming convention, it is important that you have one, and that you are consistent throughout the project. That being said, as you develop naming conventions for your projects, all the conventions should be documented! This documentation helps onboard collaborators to your project.\n\nThere are several guides available to help you establish naming conventions including here and here."
  },
  {
    "objectID": "notes/02-project-organization.html#adding-content-to-readme",
    "href": "notes/02-project-organization.html#adding-content-to-readme",
    "title": "Project Organization",
    "section": "Adding Content to README",
    "text": "Adding Content to README\nSince README files are plain text files, they cannot include formatting like bold or italic. However, they do typically include Markdown syntax (which is itself plain text). The plain text nature of these files keeps them small in size and accessible to anyone with any type of computer.\n\nThere are several online guides for what to include in a README file, including here and here. There is also a pretty good template for a README for data science oriented projects here.\n\nSince the README file is informational, you can include any type of information in this file that is useful to the project. For example, you could add your naming conventions to this file.\n# assignment-01\n\nThis directory contains all of the files necessary to complete Assignment 1.\n\n\n# Naming Conventions\n\n- File names should be short but descriptive (less than 25 characters)\n- All lowercase letters\n- Avoid special characters and spaces in a file name\n  + Use hyphens instead of spaces to separate words (e.g., `assignment-01`)\n- Any names that include the date will use the ISO 8601 date format (YYYYMMDD)\n- Any names that include a number will include at least two digits (e.g., `assignment-01` rather than `assignment-1`)\n\nWhile you should only have one README file per directory, you can have different README files in other directories. For example, you could create a README file in the data directory that includes the codebook information for your data files."
  },
  {
    "objectID": "notes/03-creating-tables.html#column-labels",
    "href": "notes/03-creating-tables.html#column-labels",
    "title": "Creating Tables using the {gt} Package",
    "section": "Column Labels",
    "text": "Column Labels\nColumn labels can be changed from the names of the columns used in the data frame. To change them we will use the cols_label() function. This function takes as many arguments as there are columns, each mapping a label to the original column name. Below, we change our column names to match those in Table 1.\n\n# Change Column labels\ntab_01 |>\n  gt() |>\n  cols_label(\n    variable = \"Variable\",\n    m = \"M\",\n    sd = \"SD\",\n    min = \"Min.\",\n    max = \"Max.\"\n  )\n\n\n\n\n\n  \n  \n    \n      Variable\n      M\n      SD\n      Min.\n      Max.\n    \n  \n  \n    Environmental policy strength\n17.60\n8.23\n4.00\n37.00\n    Corruption\n0.32\n0.22\n0.00\n0.98\n    Wealth\n28.15\n36.38\n12.78\n278.01\n    Toxic waste severity\n3.53\n1.14\n0.00\n5.76\n    Democratic control\n0.63\n0.26\n0.00\n1.00\n    Interparty competition\n39.03\n11.40\n9.26\n56.58\n    Public environmentalism\n2.49\n0.10\n2.31\n2.70\n  \n  \n  \n\n\n\n\nWe can also use the md() function to include Markdown syntax to further format our labels. For example, to make the column labels italics we use the following.\n\n# Change Column labels to italics\ntab_01 |>\n  gt() |>\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) \n\n\n\n\n\n  \n  \n    \n      Variable\n      M\n      SD\n      Min.\n      Max.\n    \n  \n  \n    Environmental policy strength\n17.60\n8.23\n4.00\n37.00\n    Corruption\n0.32\n0.22\n0.00\n0.98\n    Wealth\n28.15\n36.38\n12.78\n278.01\n    Toxic waste severity\n3.53\n1.14\n0.00\n5.76\n    Democratic control\n0.63\n0.26\n0.00\n1.00\n    Interparty competition\n39.03\n11.40\n9.26\n56.58\n    Public environmentalism\n2.49\n0.10\n2.31\n2.70\n  \n  \n  \n\n\n\n\nThe names we just gave to the variables are only labels. As we refer to the columns in additional functions, we need to use their original names from the data frame."
  },
  {
    "objectID": "notes/03-creating-tables.html#column-alignment",
    "href": "notes/03-creating-tables.html#column-alignment",
    "title": "Creating Tables using the {gt} Package",
    "section": "Column Alignment",
    "text": "Column Alignment\nTo change the column alignment, we use the cols_align() function. We provide this with two arguments. The columns= argument takes a vector of column names using the c() function, and the align= argument takes a character string of \"left\", \"right\", or \"center\". Following typical formatting rules, we left align text columns and center numerical columns.\n\n# Change Column labels to italics\ntab_01 |>\n  gt() |>\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |>\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |>\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  )\n\n\n\n\n\n  \n  \n    \n      Variable\n      M\n      SD\n      Min.\n      Max.\n    \n  \n  \n    Environmental policy strength\n17.60\n8.23\n4.00\n37.00\n    Corruption\n0.32\n0.22\n0.00\n0.98\n    Wealth\n28.15\n36.38\n12.78\n278.01\n    Toxic waste severity\n3.53\n1.14\n0.00\n5.76\n    Democratic control\n0.63\n0.26\n0.00\n1.00\n    Interparty competition\n39.03\n11.40\n9.26\n56.58\n    Public environmentalism\n2.49\n0.10\n2.31\n2.70"
  },
  {
    "objectID": "notes/03-creating-tables.html#adding-a-title-and-subtitle",
    "href": "notes/03-creating-tables.html#adding-a-title-and-subtitle",
    "title": "Creating Tables using the {gt} Package",
    "section": "Adding a Title and Subtitle",
    "text": "Adding a Title and Subtitle\nThe tab_header() function can be used to add a title or subtitle to your table. Here we again use the md() function to allow us to use Markdown syntax directly in the title. I also use the opt_align_table_header() function to left align the title and subtitle per APA.\n\n# Add title and subtitle\ntab_01 |>\n  gt() |>\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |>\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |>\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  ) |>\n    tab_header(\n    title = md(\"**Table 1.** Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\"),\n  ) |>\n  opt_align_table_header(\"left\")\n\n\n\n\n\n  \n    \n      Table 1. Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\n    \n    \n  \n  \n    \n      Variable\n      M\n      SD\n      Min.\n      Max.\n    \n  \n  \n    Environmental policy strength\n17.60\n8.23\n4.00\n37.00\n    Corruption\n0.32\n0.22\n0.00\n0.98\n    Wealth\n28.15\n36.38\n12.78\n278.01\n    Toxic waste severity\n3.53\n1.14\n0.00\n5.76\n    Democratic control\n0.63\n0.26\n0.00\n1.00\n    Interparty competition\n39.03\n11.40\n9.26\n56.58\n    Public environmentalism\n2.49\n0.10\n2.31\n2.70\n  \n  \n  \n\n\n\n\nSince the title and subtitle appear on separate lines, you can take advantage of that to use the title to provide the table number and the subtitle provides the table caption if you are trying to format in APA style."
  },
  {
    "objectID": "notes/03-creating-tables.html#fine-tuning-the-table",
    "href": "notes/03-creating-tables.html#fine-tuning-the-table",
    "title": "Creating Tables using the {gt} Package",
    "section": "Fine-Tuning the Table",
    "text": "Fine-Tuning the Table\nThe table is very close to matching the original Table 1. But, there are still a couple of things (if you are an Enneagram One) that we need to attend to. For example, we could remove the horizontal lines in the table. These lines are called “borders” and we can modify them in the tab_style() function. This is a general function that allows us to customize many parts of the table (akin to theme() in ggplot()).\nTo do this we use the style= argument and call the cell_borders() function within tab_style(). Here we remove all borders (top, bottom, left, and right) by using sides=\"all\" and setting style=NULL. The tab_style() function also requires the argument locations=. We give this argument the function cell_body() which we provide the column and row numbers that we want to remove the borders from. Since we want to keep the horizontal border associated with the first and last rows, we omit those row numbers from the rows= argument.\n\n# Add title and subtitle\ntab_01 |>\n  gt() |>\n  cols_label(\n    variable = md(\"*Variable*\"),\n    m = md(\"*M*\"),\n    sd = md(\"*SD*\"),\n    min = md(\"*Min.*\"),\n    max = md(\"*Max.*\")\n  ) |>\n  cols_align(\n    columns = c(variable),\n    align = \"left\"\n  ) |>\n  cols_align(\n    columns = c(m, sd, min, max),\n    align = \"center\"\n  ) |>\n    tab_header(\n    title = md(\"**Table 1.** Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\"),\n  ) |>\n  opt_align_table_header(\"left\")  |>\n  tab_style(\n    style = cell_borders(\n      sides = \"all\", \n      style = NULL\n      ),\n    locations = cells_body(\n      columns = 1:5,\n      rows = 2:6\n    )\n  )\n\n\n\n\n\n  \n    \n      Table 1. Summary statistics for the unstandardized outcome, focal predictor, and five covariates.\n    \n    \n  \n  \n    \n      Variable\n      M\n      SD\n      Min.\n      Max.\n    \n  \n  \n    Environmental policy strength\n17.60\n8.23\n4.00\n37.00\n    Corruption\n0.32\n0.22\n0.00\n0.98\n    Wealth\n28.15\n36.38\n12.78\n278.01\n    Toxic waste severity\n3.53\n1.14\n0.00\n5.76\n    Democratic control\n0.63\n0.26\n0.00\n1.00\n    Interparty competition\n39.03\n11.40\n9.26\n56.58\n    Public environmentalism\n2.49\n0.10\n2.31\n2.70"
  },
  {
    "objectID": "notes/03-introduction-to-quarto.html",
    "href": "notes/03-introduction-to-quarto.html",
    "title": "📖 Introduction to Quarto",
    "section": "",
    "text": "In this set of notes, you will continue your Quarto journey. One important note before we begin:"
  },
  {
    "objectID": "notes/03-introduction-to-quarto.html#add-a-code-chunk-to-load-libraries-and-import-data",
    "href": "notes/03-introduction-to-quarto.html#add-a-code-chunk-to-load-libraries-and-import-data",
    "title": "📖 Introduction to Quarto",
    "section": "Add a Code Chunk to Load Libraries and Import Data",
    "text": "Add a Code Chunk to Load Libraries and Import Data\nThe first question on Assignment 1 is:\n\nImport the data and display the first several rows of data (not all of it). Use one of the paged table options in your YAML to ensure that this is printed nicely. All syntax for these commands should be hidden.\n\n\n\nCreate a Level-2 Heading in your QMD document that is called “Question 1”.\nThen create a new R chunk. In that chunk load the {tidyverse} library. Don’t forget to add a comment.\nIn the same R chunk, import the fertility.csv data into an object called fertility.\n\n\nTo import the data (which you put in the data directory of your project last class) use the following syntax:\n\nfertility = read_csv(\"data/fertility.csv\")\n\nThe path inside the quotation marks gives the location (relative to the QMD file) of the fertility.csv data. Here, the data are in the directory called data. Thus the path data/fertility.csv indicates go to the data folder and inside of that locate fertility.csv.\n\nWhat would the path be inside the quotation marks if our project had the following tree?\nassignment-01\n    ├── README\n    ├── assets\n    ├── assignment-01.qmd\n    ├── assignment-01.Rproj\n    ├── data\n       ├── assignment-data\n          └── fertility.csv\n       └── notes-data\n    ├── figs\n    └── scripts\n\n\nUse the first code chunk in your Quarto document to load all the packages and datasets used in the document. Label this chunk setup. This has the added advantage that others can immediately see what packages and datasets are needed to run the document. Labelling it setup also runs that chunk when you try to run other chunks, so your code works!"
  },
  {
    "objectID": "notes/03-introduction-to-quarto.html#displaying-data",
    "href": "notes/03-introduction-to-quarto.html#displaying-data",
    "title": "📖 Introduction to Quarto",
    "section": "Displaying Data",
    "text": "Displaying Data\n\nIn the same R chunk display the data by typing the data object name after you import the data.\n\n\nRendering the document should display the entire data set in your outputted HTML file! In general, we do not want to take the space (especially when data sets are large) to do this. One option is to change the way that data frames/tibbles are printed in the Quarto document. To do this we need to update our YAML.\n\nUpdate the YAML of your QMD to the following:\n---\ntitle: \"Assignment 1\"\nsubtitle: \"Introduction to Quarto\"\nauthor: \"Your Group's Names\"\ndate: \"January XX, 2023\"\nformat:\n  html:\n    df-print: paged\neditor: visual\n---\nRe-render the document.\n\n\nThis sets the printing option for data frames into an HTML table that has clickable paging when the data are large. The Quarto documentation Data Frames, includes other options for printing data frames.\n\nPay attention to the spacing in the YAML!!!!"
  },
  {
    "objectID": "notes/03-introduction-to-quarto.html#add-a-code-chunk-to-create-a-plot",
    "href": "notes/03-introduction-to-quarto.html#add-a-code-chunk-to-create-a-plot",
    "title": "📖 Introduction to Quarto",
    "section": "Add a Code Chunk to Create a Plot",
    "text": "Add a Code Chunk to Create a Plot\n\nCreate another Level-1 header in your document called “Scatterplot”. Then create a code chunk in which you create a scatterplot of infant mortality rates (y-axis) versus female education level (x-axis).\n\nIt is a good idea to include a label in the chunk options for each plot you create. A label is the name of this code chunk. (Any code chunk, even if it is not a plot, can have a label.) In the plot below, the chunk’s label is fig-scatterplot.\n\n\n\n\n```{r}\n#| label: fig-scatterplot\n#| fig-cap: \"Scatterplot showing the relationship between female education level and infant mortality.\"\nggplot(data = fertility, aes(x = educ_female, y = infant_mortality)) +\n   geom_point()\n```\n\n\n\n\nFigure 1: Scatterplot showing the relationship between female education level and infant mortality.\n\n\n\n\nNotice that by labeling this chunk, the figure is numbered in the document. We also added a caption using the fig-cap: option.\n\n\nFigure Size\nThere are a couple other chunk options for figures that help set the aspect ratio and size of the figure in your document. The aspect ratio is the ratio of a figure’s width to its height. We set this using the fig-width: and fig-height: chunk options. (The default value for both is 6; thus the figure appears square.) Below we change these so that the figure appears wider than it is tall.\n\n```{r}\n#| label: fig-scatterplot-2\n#| fig-cap: \"Scatterplot showing the relationship between female education level and infant mortality.\"\n#| fig-width: 9\n#| fig-height: 5\nggplot(data = fertility, aes(x = educ_female, y = infant_mortality)) +\n   geom_point()\n```\n\n\n\n\nFigure 2: Scatterplot showing the relationship between female education level and infant mortality.\n\n\n\n\n\nIt is important to change the aspect ratio when you have legends or when you are using {patchwork} to stack or put multiple figures next to each other. Many times you need to find a good aspect ratio though trial-and-error of trying different values.\n\nThe actual size of the figure in the document is independent of the aspect ratio and can be set using the out-width: or out-height: chunk option. These options take a character string of the direct size (in HTML documents this is typically in pixels) or of the percentage of the output width/height. Here we keep the same aspect ratio, but make the figure smaller by setting the figure width to 40% of the document’s width.\n\n```{r}\n#| label: fig-scatterplot-3\n#| fig-cap: \"Scatterplot showing the relationship between female education level and infant mortality.\"\n#| fig-width: 9\n#| fig-height: 5\n#| out-width: \"40%\"\nggplot(data = fertility, aes(x = educ_female, y = infant_mortality)) +\n   geom_point()\n```\n\n\n\n\nFigure 3: Scatterplot showing the relationship between female education level and infant mortality.\n\n\n\n\n:::protip If you set the aspect ratio of the figure, you need only set out-width: or out-height:. You don’t need to set both as the other will be determined by the aspect ratio. ::"
  },
  {
    "objectID": "notes/03-introduction-to-quarto.html#inline-code-chunks-for-better-reproducibility",
    "href": "notes/03-introduction-to-quarto.html#inline-code-chunks-for-better-reproducibility",
    "title": "📖 Introduction to Quarto",
    "section": "Inline Code Chunks for Better Reproducibility",
    "text": "Inline Code Chunks for Better Reproducibility\nIn writing papers where there are results from data analyses being reported in the text, inline code chunks are boss! For example, consider writing the following sentence in an analysis of the fertility.csv data.\n\nThe mean infant mortality rate is 26.05% (SD = 23.98).\n\nRather than computing these values and then transposing the values into the sentence, we can use inline code chunks to directly compute and write the values in the sentence. Here is some syntax we could use:\nThe mean infant mortality rate is `r mean(fertility$infant_mortality)` (SD = `r sd(fertility$infant_mortality)`).\nThis produces the following sentence in the rendered document:\n\nThe mean infant mortality rate is 26.05 (SD = 23.982719).\n\n\n\nRounding\nThere are several ways to set the rounding to two decimal places. One is to embed the computation in the round() function. For example, in the first inline chunk we could use: round(mean(fertility$infant_mortality), 2).\nYou can also do this in a separate code chunk and then call the values in the inline computation (as suggested in the Quarto Computations Tutorial)."
  },
  {
    "objectID": "notes/03-introduction-to-quarto.html#better-typesetting-of-equations",
    "href": "notes/03-introduction-to-quarto.html#better-typesetting-of-equations",
    "title": "📖 Introduction to Quarto",
    "section": "Better Typesetting of Equations",
    "text": "Better Typesetting of Equations\nWhen we typeset equations, there are a couple things we should do:\n\nUse variable names that make sense to our reader, rather than their name in our dataset/R. For example “Female Education Level” is a better name than “educ_female”.\n\nThis means we will need to add spaces into our variable names in the mathematical expression. If you tried that earlier, you found out that just hitting the spacebar does not produce a space in the outputted expression. We need to include a tilde (~) every time you want a space. So to get the name “Female Education Level” we need to use the following in our equation:\n$$\nFemale~Education~Level\n$$\n\nVariable names should also be typeset using normal text rather than italic (the default in mathematical expressions).\n\nTo achieve this, we have to embed the text we want to be normal in the syntax \\mathrm{}. This typesets the text in a roman font (normal text). For example:\n$$\n\\mathrm{Female~Education~Level}\n$$\nYou can also use \\mathit{} (italics), \\mathbf{} (bold), \\mathtt{} (typewriter text), and \\mathcal{} (caligraphy; this is useful for the “N” we use to indicate a normal distribution).\n\nHyphens need special syntax since a hyphen would be interpreted as a minus sign. In typesetting, the minus sign is longer than the hyphen symbol.\n\nIf you want to include a hyphen, we need to include it in \\mbox{}. For example, to add a hyphen in our variable name to get “Female Education-Level”, we use:\n$$\n\\mathrm{Female~Education\\mbox{-}Level}\n$$\n\nUpdate your fitted equation from before to include spaces in variable names. Also be sure the variable names are written in normal font."
  },
  {
    "objectID": "notes/03-introduction-to-quarto.html#spacing-out-the-lines",
    "href": "notes/03-introduction-to-quarto.html#spacing-out-the-lines",
    "title": "📖 Introduction to Quarto",
    "section": "Spacing Out the Lines",
    "text": "Spacing Out the Lines\nYou can add space between the lines of your multiline equation by including a unit of measurement between square brackets after the double backslashes.\n$$\n\\begin{split}\n\\hat{y}_i &= 3 + 4(1) \\\\[1em]\n&= 7\n\\end{split}\n$$\nHere the resulting display equation is:\n\\[\n\\begin{split}\n\\hat{y}_i &= 3 + 4(1) \\\\[1em]\n&= 7\n\\end{split}\n\\]\nHere we have added line space of 1 em. An em is a unit for measuring the width of printed work, equal to the height of the type size being used (typically the width of the letter “m”). Other common printing units include the en (the width of the letter “n”), and the ex (the width of the letter “x”).\n\nQuestion #8 actually asks:\n\nUse a display equation to write the equation for the underlying regression model (including error and assumptions) using Greek letters, subscripts, and variable names. Also write the equation for the fitted least squares regression equation based on the output from lm(). Type these two equations in the same display equation, each on a separate line of text in your document, and align the equals signs.\n\nTry writing this multiline equation."
  },
  {
    "objectID": "notes/03-more-quarto.html",
    "href": "notes/03-more-quarto.html",
    "title": "More Quarto",
    "section": "",
    "text": "This document contains some additional instruction for EPsy 8252. Note that I might add to this as students hit me with questions over the course of the semester."
  },
  {
    "objectID": "notes/03-more-quarto.html#creating-.bib-files-using-zotero",
    "href": "notes/03-more-quarto.html#creating-.bib-files-using-zotero",
    "title": "More Quarto",
    "section": "Creating .bib Files Using Zotero",
    "text": "Creating .bib Files Using Zotero\nMost reference managers (e.g., Papers, Zotero, Mendelay) can produce BibTex files. Here I will illustrate the process using Zotero.\n\nCreate a New Collection.\nDrag the references you want in your BibTeX database into this collection. For today, drag the Carmichael (1954) and Ross, Winterhalder, & McElreath (2020) references into this collection.\nRight-click the collection and select Export Collection.\nIn the pop-up window, change the format of the exported collection to BibTex.\nClick OK.\n\nName the BibTex file (here I named it my-bibliography.bib) and save it in the assets folder.\n\n\n\n\n\n\n\n\n\nWhen you call the BibTex file in the bibliography: key of the YAML in your QMD document, you will need to give the location of the BibTex (relative to the QMD document) and the name you just gave it. For example if you are calling a bibliography in the quarto document assignment-01.qmd that has the following directory/folder structure:\nassignment-01\n    ├── README\n    ├── assets\n       └── my-bibliography.bib\n    ├── assignment-01.qmd\n    ├── assignment-01.Rproj\n    ├── data\n       └── fertility.csv\n    ├── figs\n    └── scripts"
  },
  {
    "objectID": "notes/03-more-quarto.html#including-the-bib-file-in-your-yaml",
    "href": "notes/03-more-quarto.html#including-the-bib-file-in-your-yaml",
    "title": "More Quarto",
    "section": "Including the BIB File in your YAML",
    "text": "Including the BIB File in your YAML\nYou would then include the following in your Quarto document’s YAML:\n---\ntitle: \"Assignment 1\"\nsubtitle: \"Introduction to Quarto\"\nauthor: \"Your Group's Names\"\ndate: \"January XX, 2023\"\nformat: html\neditor: visual\nbibliography: \"assets/my-bibliography.bib\"\n---"
  },
  {
    "objectID": "notes/03-more-quarto.html#bibtex-files",
    "href": "notes/03-more-quarto.html#bibtex-files",
    "title": "More Quarto",
    "section": "BibTex Files",
    "text": "BibTex Files\nBibTeX files are essentially databases that store bibliographic information in a plain-text (style-independent) file. The database includes a set of references and their metadata. Here is the raw text inside an example BibTex file that includes two articles written by Carmichael (1954) and Ross et al. (2020).\n@article{carmichael_1954,\n    title = {Laziness and the Scholarly Life},\n    volume = {78},\n    number = {4},\n    journal = {The Scientific Monthly},\n    author = {Carmichael, Leonard},\n    year = {1954},\n    pages = {208--213}\n}\n\n@article{ross_2020,\n    title = {Racial disparities in police use of deadly force against unarmed individuals persist after appropriately benchmarking shooting data on violent crime rates},\n    issn = {1948-5506, 1948-5514},\n    url = {http://journals.sagepub.com/doi/10.1177/1948550620916071},\n    doi = {10.1177/1948550620916071},\n    language = {en},\n    urldate = {2020-06-24},\n    journal = {Social Psychological and Personality Science},\n    author = {Ross, Cody T. and Winterhalder, Bruce and McElreath, Richard},\n    month = jun,\n    year = {2020},\n    pages = {194855062091607}\n}\nThe citation identifiers in this example are carmichael_1954 and ross_2020. These identifiers are the first bit of text after the initial curly brace in the reference. This is how we will refer to the citations in our QMD document. (Typically these are auto-generated from our reference manager.) To determine the citiation identifiers, you can open your BibTeX file in RStudio. (Do this by using Open File... within RStudio; double-clicking on the BibTeX file will likely open it in a different application.)"
  },
  {
    "objectID": "notes/03-more-quarto.html#including-citations-in-your-quarto-document",
    "href": "notes/03-more-quarto.html#including-citations-in-your-quarto-document",
    "title": "More Quarto",
    "section": "Including Citations in Your Quarto Document",
    "text": "Including Citations in Your Quarto Document\nCitations go inside square brackets and are separated by semicolons. Each citation must have a key, composed of @citation_identifier from the database (no spaces between them). For example to cite the Carmichael article:\nHere is some text and a citation [@carmichael_1954].\nThis will create a citation where you included it in the text and also add the reference at the end of the document. If you want a section header for your references, include a level-1 heading called “References” at the end of your document.\nThe rendered document now includes a citation where you added this is the QMD document. The associated reference is also included at the end of the document.\n\n\n\n\n\n\n\n\n\nCitations may also include. additional text before and after the citation. In this example we have prefixed the citation with the word “see” and added a page number after the citation. The citation identifier and the text following the citation identifier are separated by a comma.\nHere is some text and a citation [see @carmichael_1954, p. 208].\nThe output of this is:\n\nHere is some text and a citation (see Carmichael, 1954, p. 208).\n\nYou can also include multiple citations. To do this we include the different citation identifiers separated by a semicolon.\nHere are multiple citations [@ross_2020; @carmichael_1954].\nThe output of this is:\n\nHere are multiple citations (Carmichael, 1954; Ross et al., 2020).\n\nWe can also change the format of the citation. For example, here we use a format common to starting a sentence with a citation. To do that we omit the square brackets.\n@carmichael_1954 suggests something is true.\nThe output of this is:\n\nCarmichael (1954) suggests something is true.\n\n\nYou can learn more on the Citations and Footnotes Help Page in the Quarto documentation."
  },
  {
    "objectID": "notes/03-more-quarto.html#use-apa-formatted-citations-and-references",
    "href": "notes/03-more-quarto.html#use-apa-formatted-citations-and-references",
    "title": "More Quarto",
    "section": "Use APA Formatted Citations and References",
    "text": "Use APA Formatted Citations and References\nBy default, citations and references are formatted using the Chicago style. To use another style, you will need to:\n\nDownload the appropriate citation style language (CSL) file. (Find many at https://zotero.org/styles)\nPlace the CSL file in the assets folder of your project’s directory.\nSpecify the name of the CSL style file in the csl: key in the QMD file’s YAML.\n\nFor example, say you have downloaded and saved the apa-single-spaced.csl file and placed it in the assets directory, giving the following directory/folder structure:\nassignment-01\n    ├── README\n    ├── assets\n       ├── apa-single-spaced.csl\n       └── my-bibliography.bib\n    ├── assignment-01.qmd\n    ├── assignment-01.Rproj\n    ├── data\n       └── fertility.csv\n    ├── figs\n    └── scripts\nYou would include the following in your YAML.\nIf, for example, you had named the BIB file my-bibliography.bib and had put this file in the assets directory, your YAML would be:\n---\ntitle: \"Assignment 1\"\nsubtitle: \"Introduction to Quarto\"\nauthor: \"Your Group's Names\"\ndate: \"January XX, 2023\"\nformat: html\neditor: visual\nbibliography: \"assets/my-bibliography.bib\"\ncsl: \"assets/apa-single-spaced.csl\"\n---\nUse of the APA CSL file not only formats the references according to APA format, but it also fixed the order of the citations in the text itself! (Even though we included the Ross citation identifier prior to the Carmichael citation identifier in the multiple references example, the APA CSL file put them in the text alphabetically!)"
  },
  {
    "objectID": "notes/03-more-quarto.html#citation-extras",
    "href": "notes/03-more-quarto.html#citation-extras",
    "title": "More Quarto",
    "section": "Citation Extras",
    "text": "Citation Extras\nOne R package that is extremely useful, especially if you are using the visual editor in RStudio for writing QMD documents, is {citr}. This package provides functionality and an RStudio Add-In to search a BibTeX-file to create and insert formatted Markdown citations into a QMD document.\n\n\n\n\n\n\n\n\n\nThe {citr} package is only available via GitHub, so you will need to install it using{remotes}or{devtools}. (See the Computational Toolkit book for a reminder on how to do this.) The directions for installing{citr}` are also on the citr webpage along with instructions for using the package."
  },
  {
    "objectID": "notes/04-likelihood-evidence.html#an-example-of-computing-and-evaluating-likelihood",
    "href": "notes/04-likelihood-evidence.html#an-example-of-computing-and-evaluating-likelihood",
    "title": "Likelihood: A Framework for Evidence",
    "section": "An Example of Computing and Evaluating Likelihood",
    "text": "An Example of Computing and Evaluating Likelihood\nThe likelihood allows us to answer probability questions about a set of parameters. For example, what is the likelihood (probability) that the data (\\(x = \\{30, 20, 24, 27\\}\\)) were generated from a normal distribution with a mean of 20 and standard deviation of 4? To compute the likelihood we compute the joint probability density of the data under that particular set of parameters.\n\nprod(dnorm(x = c(30, 20, 24, 27), mean = 20, sd = 4))\n\n[1] 0.0000005702554\n\n\nWhat is the likelihood (probability) that the same set of data (\\(x = \\{30, 20, 24, 27\\}\\)) were generated from a normal distribution with a mean of 25 and standard deviation of 4?\n\nprod(dnorm(x = c(30, 20, 24, 27), mean = 25, sd = 4))\n\n[1] 0.00001774012\n\n\nGiven the data and the model, there is more empirical support that the parameters are \\(\\mathcal{N}(25,4^2)\\) rather than \\(\\mathcal{N}(20, 4^2)\\), because the likelihood is higher for the former set of parameters. We can compute a ratio of the two likelihoods to quantify the amount of additional support for the \\(\\mathcal{N}(25,4^2)\\).\n\\[\n\\begin{split}\n\\mathrm{Likelihood~Ratio} &= \\frac{0.00001774012}{0.0000005702554} \\\\[1ex]\n&= 31.11\n\\end{split}\n\\]\nThe empirical support for the \\(\\mathcal{N}(25,4^2)\\) parameterization is 31 times that of the \\(\\mathcal{N}(20, 4^2)\\) parameterization! In a practical setting, this would lead us to adopt a mean of 25 over a mean of 20."
  },
  {
    "objectID": "notes/04-likelihood-evidence.html#log-likelihood",
    "href": "notes/04-likelihood-evidence.html#log-likelihood",
    "title": "Likelihood: A Framework for Evidence",
    "section": "Log-Likelihood",
    "text": "Log-Likelihood\nThe likelihood values are quite small since we are multiplying several probability densities (values between 0 and 1) together. Since it is hard to work with these smaller values, in practice, we often compute and work with the natural logarithm of the likelihood. So in our example, \\(\\mathcal{L}_1 = 4.71647 \\times 10^{-50}\\) and the log-likelihood would be:\n\n# Log-likelihood for Model 1\nlog(4.71647e-50)\n\n[1] -113.5782\n\n\nSimilarly, we can compute the log-likelihood for Model 2 as:\n\n# Log-likelihood for Model 2\nlog(5.231164e-48)\n\n[1] -108.8695\n\n\nWe typically denote log-likelihood using a scripted lower-case “l” (\\(\\mathcal{l}\\)). Here,\n\\[\n\\begin{split}\n\\mathcal{l}_1 &= -113.5782 \\\\[1ex]\n\\mathcal{l}_2 &= -108.8695 \\\\[1ex]\n\\end{split}\n\\]\nNote that the logarithm of a decimal will be negative, so the log-likelihood will be a negative value. Less negative log-likelihood values correspond to higher likelihood values, which indicate more empirical support. Here Model 2 has a less negative log-likelihood value (\\(-108\\)) than Model 1 (\\(-113\\)), which indicates there is more empirical support for Model 2 than Model 1.\nWe can also express the likelihood ratio using log-likelihoods. To do so we take the natural logarithm of the likelihood ratio. We also re-write it using the rules of logarithms from algebra.\n\\[\n\\begin{split}\n\\ln(\\mathrm{LR}) &= \\ln \\bigg(\\frac{\\mathcal{L}_2}{\\mathcal{L}_1}\\bigg) \\\\[2ex]\n&= \\ln \\big(\\mathcal{L}_2\\big) - \\ln \\big(\\mathcal{L}_1\\big)\n\\end{split}\n\\]\nThat is, we can find an equivalent relative support metric to the LR based on the log-likelihoods by computing the difference between them. For our example:\n\n# Difference in log-likelihoods\nlog(5.231164e-48) - log(4.71647e-50)\n\n[1] 4.708743\n\n# Equivalent to ln(LR)\nlog(5.231164e-48 / 4.71647e-50)\n\n[1] 4.708743\n\n\nUnfortunately, this difference doesn’t have the same interpretational value as the LR does, bcause this difference is in log-units. In order to get that interpretation back, we need to exponentiate (the reverse function of the logarithm) the difference:\n\n# Exponentiate the difference in log-likelihoods\nexp(4.708743)\n\n[1] 110.9127\n\n\nModel 2 has 110.9 times the empirical support than Model 1.\n\nMathematics of Log-Likelihood\nWe can express the log-likelihood of the regression residuals mathematically by taking the natural logarithm of the likelihood we computed earlier:\n\\[\n\\begin{split}\n\\ln \\Bigl(\\mathcal{L}(\\beta_0, \\beta_1 | \\mathrm{data})\\Bigr) &= \\ln \\Biggl( \\left[ \\frac{1}{\\sigma_{\\epsilon}\\sqrt{2\\pi}} \\right]^n \\times \\exp\\left[-\\frac{\\epsilon_1^2}{2\\sigma^2_{\\epsilon}}\\right] \\times  \\\\\n&~~~~~~ \\exp\\left[-\\frac{\\epsilon_2^2}{2\\sigma^2_{\\epsilon}}\\right] \\times \\ldots \\times \\exp\\left[-\\frac{\\epsilon_n^2}{2\\sigma^2_{\\epsilon}}\\right] \\Biggr) \\\\\n\\end{split}\n\\]\nUsing our rules for logarithms and re-arranging gives,\n\\[\n\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) = -\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\epsilon_i^2\n\\]\nExamining this equation, we see that the log-likelihood is a function of \\(n\\), \\(\\sigma^2_{\\epsilon}\\) and the sum of squared residuals (SSR)1. We can of course, re-express this using the the regression parameters:\n\\[\n\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) = -\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\big[Y_i - \\beta_0 - \\beta_1(X_i)\\big]^2\n\\]\nAnd, again, since \\(\\sigma^2_{\\epsilon}\\) is a function of the regression coefficients and \\(n\\), this means that the only variables in the log-likelihood function are the coefficients."
  },
  {
    "objectID": "notes/04-likelihood-evidence.html#shortcut-the-loglik-function",
    "href": "notes/04-likelihood-evidence.html#shortcut-the-loglik-function",
    "title": "Likelihood: A Framework for Evidence",
    "section": "Shortcut: The logLik() Function",
    "text": "Shortcut: The logLik() Function\nThe logLik() function can be used to obtain the log-likelihood directly from a fitted model object. For example, to find the log-likelihood for Model 1, we can use:\n\n# Compute log-likelihood for Model 1\nlogLik(lm.1)\n\n'log Lik.' -113.5472 (df=3)\n\n\nThe df output tells us how many total parameters are being estimated in the model. In our case this is three (\\(\\beta_0\\), \\(\\beta_{\\mathrm{SAT}}\\), and \\(\\sigma^2_{\\epsilon}\\)). What is more important to us currently, is the log-likelihood value; \\(\\mathcal{l}_1=-113.55\\).\nThis value is slightly different than the log-likelihood we just computed of \\(-113.58\\). This is not because of rounding in this case. It has to do with how the model is being estimated; the logLik() function assumes the parameters are being estimated using maximum likelihood (ML) rather than ordinary least squares (OLS). We will learn more about this in the next set of notes, but for now, we will just use logLik() to compute the log-likelihood.\nHere we compute the log-likelihood for Model 2 using the logLik() function. We also use the output to compute the likelihood, and the likelihood ratio between Model 2 and Model 1\n\n# Compute log-likelihood for Model 2\nlogLik(lm.2)\n\n'log Lik.' -108.7964 (df=4)\n\n# Compute likelihood for Model 2\nexp(logLik(lm.2)[1])\n\n[1] 5.627352e-48\n\n# Compute LR\nexp( logLik(lm.2)[1] - logLik(lm.1)[1] )\n\n[1] 115.6734\n\n\nBecause the output from logLik() includes extraneous information (e.g., df), we use indexing (square brackets) to extract only the part of the output we want. In this case, the [1] extracts the log-likelihood value from the logLik() output (ignoring the df part).\nAlso of note is that the df for Model 2 is four, indicating that this model is estimating four parameters (\\(\\beta_0\\), \\(\\beta_{\\mathrm{SAT}}\\), \\(\\beta_{\\mathrm{Sector}}\\), and \\(\\sigma^2_{\\epsilon}\\)). The value of df in the logLik() output is a quantification of the model’s complexity. Here Model 2 (df = 4) is more complex than Model 1 (df = 3).\nAs we consider using the likelihood ratio (LR) or the difference in log-likelihoods for model selection, we also need to consider the model complexity. In our example, the likelihood ratio of 115.7 (computed using logLik()) indicates that Model 2 has approximately 116 times the empirical support than Model 1. But, Model 2 is more complex than Model 1, so we would expect that it would be more empirically supported.\nIn this case, with a LR of 116, it seems like the data certainly support adopting Model 2 over Model 1, despite the added complexity of Model 2. But what if the LR was 10? Would that be enough additional support to warrant adopting Model 2 over Model 1? What about a LR of 5?"
  },
  {
    "objectID": "notes/04-likelihood-evidence.html#hypothesis-test-of-the-lrt",
    "href": "notes/04-likelihood-evidence.html#hypothesis-test-of-the-lrt",
    "title": "Likelihood: A Framework for Evidence",
    "section": "Hypothesis Test of the LRT",
    "text": "Hypothesis Test of the LRT\nWhen we have nested models we can carry out a hypothesis test to decide between the following competing hypotheses:\n\\[\n\\begin{split}\nH_0:& ~\\theta_0 = \\{\\beta_0,~\\beta_{\\mathrm{SAT}},~\\sigma^2_{\\epsilon}\\}\\\\[1ex]\nH_A:& \\theta_1 = \\{\\beta_0,~\\beta_{\\mathrm{SAT}},~\\beta_{\\mathrm{Sector}},~\\sigma^2_{\\epsilon}\\}\n\\end{split}\n\\]\nwhere \\(\\theta_0\\) refers to the simpler model and \\(\\theta_1\\) refers to the more complex model. This translates to adopting either the simpler model (fail to reject \\(H_0\\)) or the more complex model (reject \\(H_0\\)). To carry out this test, we translate our likelihood ratio to a test statistic called \\(\\chi^2\\) (pronounced chi-squared):\n\\[\n\\chi^2 = -2 \\ln \\bigg(\\frac{\\mathcal{L}({\\theta_0})}{\\mathcal{L}({\\theta_1})}\\bigg)\n\\]\nThat is we compute \\(-2\\) times the log of the likelihood ratio where the likelihood for the simpler model is in the numerator. (Note this is the inverse of how we have been computing the likelihood ratio!) Equivalently, we can compute this as:\n\\[\n\\chi^2 = -2 \\bigg(\\ln \\bigg[\\mathcal{L}({\\theta_0})\\bigg] - \\ln \\bigg[\\mathcal{L}({\\theta_1})\\bigg]\\bigg)\n\\]\nFor our example, we compute this using the\n\n# Compute chi-squared\n-2 * (logLik(lm.1)[1] - logLik(lm.2)[1])\n\n[1] 9.501542"
  },
  {
    "objectID": "notes/04-likelihood-evidence.html#mathematics-of-deviance",
    "href": "notes/04-likelihood-evidence.html#mathematics-of-deviance",
    "title": "Likelihood: A Framework for Evidence",
    "section": "Mathematics of Deviance",
    "text": "Mathematics of Deviance\nWe can express the deviance mathematically by multiplying the log-likelihood by \\(-2\\).\n\\[\n\\begin{split}\n\\mathrm{Deviance} &= -2 \\times\\mathcal{l}(\\beta_0, \\beta_1 | \\mathrm{data}) \\\\[1ex]\n&= -2 \\bigg(-\\frac{n}{2} \\times \\ln (2\\pi\\sigma^2_{\\epsilon}) - \\frac{1}{2\\sigma^2_{\\epsilon}} \\times \\sum \\epsilon_i^2\\bigg) \\\\[1ex]\n&= -n\\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{1}{\\sigma^2_{\\epsilon}}\\sum \\epsilon_i^2 \\\\[1ex]\n&= -n\\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{\\mathrm{RSS}}{\\sigma^2_{\\epsilon}}\n\\end{split}\n\\]\nRewriting this using the parameters from the likelihood:\n\\[\n\\mathrm{Deviance} = -n\\ln (2\\pi\\sigma^2_{\\epsilon}) + \\frac{\\sum_{i=1}^n \\big[Y_i-\\beta_0-\\beta_1(X_i)\\big]^2}{\\sigma^2_{\\epsilon}}\n\\]\nOnce again, we find that the only variables in the deviance function are the regression coefficients."
  },
  {
    "objectID": "notes/04-likelihood-evidence.html#modeling-the-variation-in-the-test-statistic",
    "href": "notes/04-likelihood-evidence.html#modeling-the-variation-in-the-test-statistic",
    "title": "Likelihood: A Framework for Evidence",
    "section": "Modeling the Variation in the Test Statistic",
    "text": "Modeling the Variation in the Test Statistic\nIf the null hypothesis is true, the difference in deviances can be modeled using a \\(\\chi^2\\)-distribution. The degrees-of-freedom for this \\(\\chi^2\\)-distribution is based on the difference in the number of parameters between the complex and simpler model. In our case this difference is 1 (\\(4-3=1\\)):\n\\[\n\\chi^2(1) = 9.50\n\\]\n\n\nCode\n# Create dataset\nfig_01 = data.frame(\n  X = seq(from = 0, to = 20, by = 0.01)\n  ) %>%\n  mutate(\n    Y = dchisq(x = X, df = 1)\n    )\n\n# Filter out X<=65\nshaded = fig_01 %>%\n  filter(X >=9.5015)\n\n# Create plot\nggplot(data = fig_01, aes(x = X, y = Y)) +\n  geom_line() +\n  xlab(\"Chi-squared\") +\n  ylab(\"Probability density\") +\n  theme_light() +\n  geom_ribbon(data = shaded, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4)\n\n\n\n\n\nFigure 1: Plot of the probability density function (PDF) for a \\(\\chi^2(1)\\) distribution. The grey shaded area represents the p-value based on \\(\\chi^2=9.5015\\).\n\n\n\n\nTo compute the p-value we use the pchisq() function.\n\n# Compute p-value for X^2 = 9.5015\n1 - pchisq(q = 9.5015, df = 1)\n\n[1] 0.00205304\n\n# Alternative method\npchisq(q = 9.5015, df = 1, lower.tail = FALSE)\n\n[1] 0.00205304\n\n\nBased on the p-value, we would reject the null hypothesis for the likelihood ratio test, which suggests that we should adopt the more complex model (Model 2)."
  },
  {
    "objectID": "readings/01-welcome-to-8252.html",
    "href": "readings/01-welcome-to-8252.html",
    "title": "📖 Welcome to EPsy 8264",
    "section": "",
    "text": "Here are some things to do before the first class:\n\nDownload and read through the course syllabus\nFamiliarize yourself with the course website\nUpdate R to the most recent version (v. 4.2.2; Innocent and Trusting)\nUpdate RStudio Desktop to the most recent version (v. 2022.12.0+353)\nUpdate all of your R packages (In the Packages tab in RStudio click Update)\nUpdate the {educate} package (to at least v.0.3.0.1). This is a github package, so see the instructions in Installing Packages from GitHub\n\nWe will also start Day 1 with a short review of the regression content from EPsy 8251.\n\nRemind yourself of the EPsy 8251 content\n\nComputational Toolkit for Educational Scientists\nStatistical Modeling and Computation for Educational Scientists"
  },
  {
    "objectID": "readings/02-project-organization.html",
    "href": "readings/02-project-organization.html",
    "title": "📖 Project Management",
    "section": "",
    "text": "Before next class, look at your computer files and your organization of those files. Here are some things to reflect on:\n\nAre your files organized into folders/directories? Or are they all in your Downloads folder?\n\nHow did you organize all the data files, notes, etc. from EPsy 8251?\n\nIf I asked you to find a specific file, could you locate it without using “Search”?\nCan you tell what is in a particular file by just looking at its name?\nDo your file names contain spaces? What about characters that aren’t letters, numbers, dashes, or underscores?\nAre your file names consistent (all lower case letters, or all title case)? Or are they all different?"
  },
  {
    "objectID": "readings/03-introduction-to-quarto-day-02.html",
    "href": "readings/03-introduction-to-quarto-day-02.html",
    "title": "Introduction to Quarto (Day 2)",
    "section": "",
    "text": "For the next class you will need a citation manager. If you don’t have a citation manager, I strongly recommend Zotero.\n\nInstall Zotero.\n\nIf you use a different citation manager, you will need to figure out how to export a .BIB file from your citation manager."
  },
  {
    "objectID": "readings/03-introduction-to-quarto-day-02.html#download-and-import-two-papers",
    "href": "readings/03-introduction-to-quarto-day-02.html#download-and-import-two-papers",
    "title": "Introduction to Quarto (Day 2)",
    "section": "Download and Import Two Papers",
    "text": "Download and Import Two Papers\nDownload the following two papers from the UMN library:\n\nCarmichael, L. (1954). Laziness and the scholarly life. The Scientific Monthly, 78(4), 208–213.\nRoss, C. T., Winterhalder, B., & McElreath, R. (2020). Racial disparities in police use of deadly force against unarmed individuals persist after appropriately benchmarking shooting data on violent crime rates. Social Psychological and Personality Science, 194855062091607. https://doi.org/10.1177/1948550620916071\n\nImport both of these into Zotero by dragging the PDFs into Zotero. After they are imported, check that all the metadata is correct. (Page numbers often need to be updated in Zotero.)"
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html",
    "href": "readings/03-introduction-to-quarto.html",
    "title": "Introduction to Quarto",
    "section": "",
    "text": "Head over to https://quarto.org/ and click on the big, blue “Get Started” button. Then follow the instructions to install Quarto.\nOpen RStudio. (Note if RStudio was open when you installed Quarto, quit and re-open it.)\n\nAt this point in RStudio, if you go to File > New File... you should see options to choose Quarto Document and Quarto Presentation."
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html#help-and-tutorials",
    "href": "readings/03-introduction-to-quarto.html#help-and-tutorials",
    "title": "Introduction to Quarto",
    "section": "Help and Tutorials",
    "text": "Help and Tutorials\nThere are several guides and tutorials available on the Quarto website. These include:\n\nCreating basic content (e.g., figures, tables, diagrams, footnotes, citations)\nIncluding computation/syntax\nGenerating output in different formats (e.g., HTML, PDF, MS Word)\nCreating slides and presentations (These notes are a Quarto presentation!)\nBuilding websites\nBuilding books\nCreating interactive dashboards"
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html#create-your-first-quarto-document",
    "href": "readings/03-introduction-to-quarto.html#create-your-first-quarto-document",
    "title": "Introduction to Quarto",
    "section": "Create Your First Quarto Document",
    "text": "Create Your First Quarto Document\nClick the Tutorial: Hello Quarto link on the left-side of the “Getting Started” page. Make sure that the RStudio tool is selected at the top of the tutorial.\n\nWork through this tutorial. This will introduce you to some of the basic concepts of creating and rendering a Quarto document."
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html#learning-how-to-integrate-r-code",
    "href": "readings/03-introduction-to-quarto.html#learning-how-to-integrate-r-code",
    "title": "Introduction to Quarto",
    "section": "Learning How to Integrate R Code",
    "text": "Learning How to Integrate R Code\nAlso work through the Tutorial: Computations. Again, before starting, ensure that the RStudio tool is selected at the top of the tutorial."
  },
  {
    "objectID": "readings/03-introduction-to-quarto.html#learn-more",
    "href": "readings/03-introduction-to-quarto.html#learn-more",
    "title": "Introduction to Quarto",
    "section": "Learn More",
    "text": "Learn More\nCheck out the Welcome to Quarto Workshop. This is a 2 1/2 hour workshop introducing people to Quarto.\n\nWelcome to Quarto Workshop! (YouTube Video)\n\nIf you are familiar with RMarkdown and want to learn how to switch over to Quarto, here is a nice 30 minute talk by Mine Çetinkaya-Rundel that helps you do that.\n\n2022 Toronto Workshop on Reproducibility - Mine Çetinkaya-Rundel (YouTube Video)"
  },
  {
    "objectID": "readings/04-probability-distributions.html",
    "href": "readings/04-probability-distributions.html",
    "title": "Introduction to Probability Distributions",
    "section": "",
    "text": "If you need to refresh your knowledge about probability distributions, I recommend reading Section 3.1.1: (Probability Basics) in Fox (2009). You could also go through the Kahn Academy: Random Variables and Probability Distributions tutorial.\nHere is some ideas you will need to be familiar with from those readings/tutorials:\nBelow, I introduce some ideas about probability distributions (especially continuous probability distributions) that will be useful for understanding likelihood."
  },
  {
    "objectID": "readings/04-probability-distributions.html#probability-density",
    "href": "readings/04-probability-distributions.html#probability-density",
    "title": "Introduction to Probability Distributions",
    "section": "Probability Density",
    "text": "Probability Density\nIn a continuous distribution we also need to account be able to talk about the fact that some outcomes are more likely than other outcomes. For example, in our standard normal distribution outcomes near zero are more probable than outcomes near 1, which are more probable than outcomes near 2, etc. Since we can’t use probability to do this (remember the probability of each outcome is the same, namely 0), we use something called probability density. This is akin to a relative probability, so outcomes with a higher probability density are more likely than outcomes with a lower probability density.\nThe mapping of all the outcomes to their probability densities is called a probability density function (PDF). Thus the equation or “bell-shaped” curve describing the standard normal distribution in Figure 1 is technically a PDF Here are some laws governing PDFs:\n\nProbability densities are always positive.\nThe probability of an outcome x between a and b equals the integral (area under the curve) between a and b of the probability density function. That is:\n\n\\[\np(a \\leq x \\leq b) = \\int_a^b p(x) dx\n\\]\n\nThe area under the curve from negative infinity to positive infinity is 1. That is:\n\n\\[\np(-\\infty \\leq x \\leq +\\infty) = \\int_{-\\infty}^{+\\infty} p(x) = 1\n\\]\nNext we will look at the PDF for a normal distribution."
  },
  {
    "objectID": "readings/04-probability-distributions.html#other-useful-r-functions-for-working-with-normal-probability-distributions",
    "href": "readings/04-probability-distributions.html#other-useful-r-functions-for-working-with-normal-probability-distributions",
    "title": "Introduction to Probability Distributions",
    "section": "Other Useful R Functions for Working with Normal Probability Distributions",
    "text": "Other Useful R Functions for Working with Normal Probability Distributions\nWe use dnorm() when we want to compute the probability density associated with a particular x-value in a given normal distribution. There are three other functions that are quite useful for working with the normal probability distribution:\n\npnorm() : To compute the probability (area under the PDF)\nqnorm() : To compute the \\(x\\) value given a particular probability\nrnorm() : To draw a random observation from the distribution\n\nEach of these function also requires the arguments mean= and sd=. Below we will examine how to use each of these additional functions."
  },
  {
    "objectID": "readings/04-probability-distributions.html#pnorm-computing-cumulative-probability-density",
    "href": "readings/04-probability-distributions.html#pnorm-computing-cumulative-probability-density",
    "title": "Introduction to Probability Distributions",
    "section": "pnorm(): Computing Cumulative Probability Density",
    "text": "pnorm(): Computing Cumulative Probability Density\nThe function pnorm() computes the area under the PDF curve from \\(-\\infty\\) to some x-value. (Sometimes this is referred to as the cumulative probability density of x.) It is important to note that the PDF is defined such that the entire area under the curve is equal to 1. Because of this, we can also think about using area under the curve as an analog to probability in a continuous distribution.\nFor example, we might ask about the probability of observing an x-value that is less than or equal to 65 given it is from a \\(\\mathcal{N}(50,10)\\) distribution. Symbolically, we want to find:\n\\[\nP\\bigg(x \\leq 65 \\mid \\mathcal{N}(50,10)\\bigg)\n\\]\nThis is akin to finding the proportion of the area under the \\(\\mathcal{N}(50,10)\\) PDF that is to the left of 65. The figure below shows a graphical depiction of the cumulative probability density for \\(x=65\\).\n\n\nCode\n# Create dataset\nfig_03 = data.frame(\n  X = seq(from = 10, to = 90, by = 0.01)\n  ) %>% \n  mutate(\n    Y = dnorm(x = X, mean = 50, sd = 10)\n    )\n\n# Filter out X<=65\nshaded = fig_03 %>%\n  filter(X <= 65)\n\n# Create plot\nggplot(data = fig_03, aes(x = X, y = Y)) +\n  geom_ribbon(data = shaded, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"X\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\nFigure 3: Plot of the probability density function (PDF) for a \\(\\mathcal{N}(50,10)\\) distribution. The area that is shaded grey (relative to the total area under the PDF) represents the cumulative probability density for \\(x=65\\).\n\n\n\n\nWe can compute the cumulative probability density using the pnorm() function. The “p” stand for “probability”.\n\n# Find P(x<=65 | N(50,10) )\npnorm(q = 65, mean = 50, sd = 10)\n\n[1] 0.9331928\n\n\nWe can interpret this as:\n\nThe probability of observing an x-value that is less than or equal to 65 (if it is drawn from a normal distribution with a mean of 50 and standard deviation of 10) is 0.933.\n\nIn mathematics, the area under a curve is called an integral. The grey-shaded area in the previous figure can also be expressed as an integral of the probability density function:\n\\[\n\\int_{-\\infty}^{65} p(x) dx\n\\]\nwhere \\(p(x)\\) is the PDF for the normal distribution.\nThe most common application for finding the cumulative density is to compute a p-value. The p-value is just the area under the distribution (curve) that is AT LEAST as extreme as some observed value. For example, assume we computed a test statistic of \\(z=2.5\\), and were evaluating whether this was different from 0 (two-tailed test). Graphically, we want to determine the proportion of the area under the PDF that is shaded grey in the figure below.\n\n\nCode\n# Create data\nfig_04 = data.frame(\n  X = seq(from = -4, to = 4, by = 0.01)\n  ) %>% \n  mutate(\n    Y = dnorm(x = X, mean = 0, sd = 1)\n    )\n\n# Filter data for shading\nshade_01 = fig_04 %>%\n  filter(X >= 2.5)\n\nshade_02 = fig_04 %>%\n  filter(X <= -2.5)\n\n# Create plot\nggplot(data = fig_04, aes(x = X, y = Y)) +\n  geom_ribbon(data = shade_01, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_ribbon(data = shade_02, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4) +\n  geom_line() +\n  xlab(\"z\") +\n  ylab(\"Probability density\") +\n  theme_light()\n\n\n\n\n\nFigure 4: Plot of the probability density function (PDF) for the standard normal distribution (\\(M=0\\), \\(SD=1\\)). The cumulative density representing the p-value for a two-tailed test evaluating whether \\(\\mu=0\\) using an observed mean of 2.5 is also displayed.\n\n\n\n\nIf the distribution of the test statistic is normally distributed, we can use pnorm() to compute the p-value. If we assume the test statistic, z, has been scaled to use standardized units, the standard deviation we use in pnorm() will be sd=1. The mean is based on the value being tested in the null hypothesis. In most null hypotheses, we are testing a difference from 0 (e.g., \\(H_0: \\mu=0\\), \\(H_0: \\beta=0\\)), so we would use mean=0 in the pnorm() function.\nRemember, pnorm() computes the proportion of the area under the curve TO THE LEFT of a particular value. Here we will compute the area to the left of \\(-2.5\\) and then double it to produce the actual p-value. (We can double it because the normal distribution is symmetric so the area to the left of \\(-2.5\\) is the same as the area to the right of \\(+2.5\\).)\n\n# Compute the p-value based on z=2.5\n2 * pnorm(q = -2.5, mean = 0, sd = 1)\n\n[1] 0.01241933\n\n\nWe interpret this p-value as:\n\nThe probability of observing a statistic at least as extreme as 2.5, assuming the null hypothesis is true, is 0.012. This is evidence against the null hypothesis since the data are inconsistent with the assumed hypothesis.\n\n\n\nqnorm(): Computing Quantiles\nThe qnorm() function is essentially the inverse of the pnorm() function. The pnorm() function computes the cumulative probability GIVEN a particular quantile (x-value). The qnorm() function computes the quantile GIVEN a cumulative probability. For example, in the \\(\\mathcal{N}(50, 10)\\) distribution, half of the area under the PDF is below the x-value (quantile) of 50.\nTo use the qnorm() function to give the x-value (quantile) that defines the lower 0.5 of the area under the \\(\\mathcal{N}(50, 10)\\) PDF, the syntax would be:\n\n# Find the quantile that has a cumulative density of 0.5 in the N(50, 10) distribution\nqnorm(p = 0.5, mean = 50, sd = 10)\n\n[1] 50\n\n\n\n\n\nrnorm(): Generating Random Observations\nThe rnorm() function can be used to generate random observations drawn from a specified normal distribution. Aside from the mean= and sd= arguments, we also need to specify the number of observations to generate by including the argument n=. For example, to generate 15 observations drawn from a \\(\\mathcal{N}(50,10)\\) distribution we would use the following syntax:\n\n# Generate 15 observations from N(50,10)\nset.seed(100)\nrnorm(n = 15, mean = 50, sd = 10)\n\n [1] 44.97808 51.31531 49.21083 58.86785 51.16971 53.18630 44.18209 57.14533\n [9] 41.74741 46.40138 50.89886 50.96274 47.98366 57.39840 51.23380\n\n\nThe set.seed() function sets the state of the random number generator used in R so that the results are reproducible. If you don’t use set.seed() you will get a different set of observations each time you run rnorm(). Here we set the starting seed to 100, but you can set this to any integer you want."
  },
  {
    "objectID": "readings/04-probability-distributions.html#computing-f-from-the-anova-partitioning",
    "href": "readings/04-probability-distributions.html#computing-f-from-the-anova-partitioning",
    "title": "Introduction to Probability Distributions",
    "section": "Computing F from the ANOVA Partitioning",
    "text": "Computing F from the ANOVA Partitioning\nWe can also compute the model-level F-statistic directly using the partitioning of variation from the ANOVA table.\n\n# Partition the variation\nanova(lm.1)\n\n\n\n  \n\n\n\nThe F-statistic is a ratio of the mean square for the model and the mean square for the error. To compute a mean square we use the general formula:\n\\[\n\\mathrm{MS} = \\frac{\\mathrm{SS}}{\\mathrm{df}}\n\\]\nThe model includes both the education and seniority predictor, so we combine the SS and df. The MS model is:\n\\[\n\\begin{split}\n\\mathrm{MS}_{\\mathrm{Model}} &= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{4147.3 + 722.9}{1 + 1} \\\\[1ex]\n&= \\frac{4870.2}{2} \\\\[1ex]\n&= 2435.1\n\\end{split}\n\\]\nThe MS error is:\n\\[\n\\begin{split}\n\\mathrm{MS}_{\\mathrm{Error}} &= \\frac{\\mathrm{SS}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\frac{1695.3 }{29} \\\\[1ex]\n&= 58.5\n\\end{split}\n\\]\nThen, we compute the F-statistic by computing the ratio of these two mean squares.\n\\[\n\\begin{split}\nF &= \\frac{\\mathrm{MS}_{\\mathrm{Model}}}{\\mathrm{MS}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\frac{2435.1}{58.5} \\\\[1ex]\n&= 41.6\n\\end{split}\n\\]\nSince a mean square represents the average amount of variation (per degree of freedom), we can see that F is a ratio between the average amount of variation explained by the model and the average amount of variation unexplained by the model. In our example, this ratio is 41.6; on average the model explains 41.6 times the variation that is unexplained.\nNote that this is an identical computation (although reframed) as the initial computation for F. We can use mathematics to show this equivalence:\n\\[\n\\begin{split}\nF &= \\frac{R^2}{1-R^2} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{SS}_{\\mathrm{Total}}}}{\\frac{\\mathrm{SS}_{\\mathrm{Error}}}{\\mathrm{SS}_{\\mathrm{Total}}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{SS}_{\\mathrm{Error}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{df}_{\\mathrm{Model}}} \\\\[1ex]\n&= \\frac{\\mathrm{SS}_{\\mathrm{Model}}}{\\mathrm{df}_{\\mathrm{Model}}} \\times \\frac{\\mathrm{df}_{\\mathrm{Error}}}{\\mathrm{SS}_{\\mathrm{Error}}} \\\\[1ex]\n&= \\mathrm{MS}_{\\mathrm{Model}} \\times \\frac{1}{\\mathrm{MS}_{\\mathrm{Error}}}\\\\[1ex]\n&= \\frac{\\mathrm{MS}_{\\mathrm{Model}}}{\\mathrm{MS}_{\\mathrm{Error}}}\n\\end{split}\n\\]"
  },
  {
    "objectID": "readings/04-probability-distributions.html#testing-the-model-level-null-hypothesis",
    "href": "readings/04-probability-distributions.html#testing-the-model-level-null-hypothesis",
    "title": "Introduction to Probability Distributions",
    "section": "Testing the Model-Level Null Hypothesis",
    "text": "Testing the Model-Level Null Hypothesis\nWe evaluate our test statistic (F in this case) in the appropriate test distribution, in this case an F-distribution with 2 and 29 degrees of freedom. The figure below, shows the \\(F(2,29)\\)-distribution as a solid, black line. The p-value is the area under the curve that is at least as extreme as the observed F-value of 41.7.\n\n\nCode\n# Create data\nfig_11 = data.frame(\n  X = seq(from = 0, to = 50, by = 0.01)\n  ) %>%\n  mutate(\n    Y = df(x = X, df1 = 2, df2 = 29)\n    )\n\n# Filter shaded area\nshade = fig_11 %>%\n  filter(X >= 41.7)\n\n# Create plot\nggplot(data = fig_11, aes(x = X, y = Y)) +\n  geom_line() +\n  theme_bw() +\n  geom_ribbon(data = shade, ymin = -10, aes(ymax = Y), color = \"#bbbbbb\", alpha = 0.4)\n\n\n\n\n\nFigure 11: Plot of the probability density function (PDF) for the \\(F(2,~29)\\)-distribution. The cumulative density representing the p-value for a test evaluating whether \\(\\rho^2=0\\) using an observed F-statistic of 41.7 is also displayed.\n\n\n\n\nThe computation using the cumulative density function, pf(), to obtain the p-value is:\n\n# p-value for F(2,29)=41.7\n1 - pf(41.7, df1 = 2, df2 = 29)\n\n[1] 0.000000002942114\n\n\nBecause we want the upper-tail, rather than taking the difference from 1, we can also use the lower.tail=FALSE argument in pf().\n\n# p-value for F(2,29)=41.7\npf(41.7, df1 = 2, df2 = 29, lower.tail = FALSE)\n\n[1] 0.000000002942114"
  },
  {
    "objectID": "readings/04-probability-distributions.html#mean-squares-are-variance-estimates",
    "href": "readings/04-probability-distributions.html#mean-squares-are-variance-estimates",
    "title": "Introduction to Probability Distributions",
    "section": "Mean Squares are Variance Estimates",
    "text": "Mean Squares are Variance Estimates\nMean squares are also estimates of the variance. Consider the computational formula for the sample variance,\n\\[\n\\hat{\\sigma}^2 = \\frac{\\sum(Y - \\bar{Y})^2}{n-1}\n\\]\nThis is the total sum of squares divided by the total df. The variance of the outcome variable is interpreted as the average amount of variation in the outcome variable (in the squared metric). Thus, it is also referred to as the mean square total.\nWhen we compute an F-statistic, we are finding the ratio of two different variance estimates—one based on the model (explained variance) and one based on the error (unexplained variance). Under the null hypothesis that \\(\\rho^2 = 0\\), we are assuming that all the variance is unexplained. In that case, our F-statistic would be close to zero. When the model explains a significant amount of variation, the numerator gets larger relative to the denominator and the F-value is larger.\nThe mean squared error (from the anova() output) plays a special role in regression analysis. It is the variance estimate for the conditional distributions of the residuals in our visual depiction of the distributional assumptions of the residuals underlying linear regression.\n\n\n\n\n\nFigure 12: Visual Depiction of the Distributional Assumptions of the Residuals Underlying Linear Regression\n\n\n\n\nRecall that we made implicit assumptions about the conditional distributions of the residuals, namely that they were identically and normally distributed with a mean of zero and some variance. Based on the estimate of the mean squared error, the variance of each of these distributions is 58.5.\nWhile the variance is a mathematical convenience, the standard deviation is often a better descriptor of the variation in a distribution since it is measured in the original metric. The standard deviation fro the residuals (error) is 7.6. Because the residuals are statistics (summaries computed from sample data), their standard deviation is referred to as a “standard error”.\n\nThe residual standard error (RSE) is sometimes referred to as the Root Mean Squared Error (RMSE).\n\n\n# Compute RMSE\nsqrt(58.5)\n\n[1] 7.648529\n\n\nWhy is this value important? It gives the expected variation in the conditional residual distributions, which is a measure of the average amount of error. For example, since all of the conditional distributions of the residuals are assumed to be normally distributed, we would expect that 95% of the residuals would fall between \\(\\pm2\\) standard errors from 0; or, in this case, between \\(-15.3\\) and \\(+15.3\\). Observations with residuals that are more extreme may be regression outliers.\nMore importantly, it is a value that we need to estimate in order to specify the model."
  },
  {
    "objectID": "readings/04-probability-distributions.html#confidencecompatibility-intervals-for-the-coefficients",
    "href": "readings/04-probability-distributions.html#confidencecompatibility-intervals-for-the-coefficients",
    "title": "Introduction to Probability Distributions",
    "section": "Confidence/Compatibility Intervals for the Coefficients",
    "text": "Confidence/Compatibility Intervals for the Coefficients\nThe confidence interval for the kth regression coefficient is computed as:\n\\[\n\\mathrm{CI} = \\hat\\beta_k \\pm t^{*}(\\mathrm{SE}_{\\hat\\beta_k})\n\\]\nwhere \\(t^*\\) is the quantile of the t-distribution that defines the confidence level for the interval. (This t-distribution, again, has degrees-of-freedom equal to the error df in the model.) The confidence level is related to the alpha level (type I error rate) used in inference. Namely,\n\\[\n\\mathrm{Confidence~Level} = 1 - \\alpha\n\\]\nSo, if you use \\(\\alpha=.05\\), then the confidence level would be \\(.95\\), and we would call this a 95% confidence interval. The alpha value also helps determine the quantile we use in the CI formula,\n\\[\nt^* = (1-\\frac{\\alpha}{2}) ~ \\mathrm{quantile}\n\\] For the example using \\(\\alpha=.05\\), a 95% confidence interval, the \\(t^*\\) value would be associated with the quantile of 0.975. We would denote this as:\n\\[\nt^{*}_{.975}\n\\]\nSay we wanted to find the 95% confidence interval for the education coefficient. We know that the estimated coefficient for education is 2.25, and the standard error for this estimate is 0.335. We also know that based on the model fitted, the residual df is 29. We need to find the 0.975th quantile in the t-distribution with 29 df.\n\n# Find 0.975th quantile\nqt(p = 0.975, df = 29)\n\n[1] 2.04523\n\n\nNow we can use all of this information to compute the confidence interval:\n\\[\n\\begin{split}\n95\\%~CI  &= 2.25 \\pm 2.04523(0.335) \\\\[1ex]\n&= \\big[1.56,~2.94\\big]\n\\end{split}\n\\]"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Calendar",
    "section": "",
    "text": "The calendar below lists the tentative course topics. The dates listed are subject to change at the instructor’s discretion.\n\n\n\n\n\n\n\n\n\nT/R\n\n\nM/W\n\n\nPrep\n\n\nTopic\n\n\nNotes\n\n\nQMD\n\n\nScript\n\n\n\n\n\n\n\n\n\n\n \n\n\nJan. 17\n\n\nJan. 18\n\n\n\n\n\nWelcome to Epsy 8252/Regression Review\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nJan. 19\n\n\nJan. 23\n\n\n\n\n\n\n\nUnit 01: Introduction to Quarto\n\n\n\n\n \n\n\nJan. 24\n\n\nJan. 25\n\n\n\n\n\nProject Organization\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nJan. 26\n\n\nJan. 30\n\n\n\n\n\nIntroduction to Quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nJan. 31\n\n\nFeb. 01\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nMore Quarto (Citations, Etc.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nCreating Tables with {gt}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 02: Model Selection and Evidence\n\n\n\n\n \n\n\nFeb. 02\n\n\nFeb. 06\n\n\n\n\n\nLikelihood: A framework for evidence\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nFeb. 07\n\n\nFeb. 08\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nOptional: Likelihood: A Framework for Estimation\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 03: Dealing with Nonlinearity\n\n\n\n\n \n\n\nFeb. 09\n\n\nFeb. 13\n\n\n\n\n\nPolynomial Effects\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nFeb. 14\n\n\nFeb. 15\n\n\n\n\nUnit 02 (Revisited): Model Selection and Evidence\n\n\n\n\n \n\n\nFeb. 16\n\n\nFeb. 20\n\n\n\n\n\nInformation Criteria and Model Selection\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nFeb. 21\n\n\nFeb. 22"
  },
  {
    "objectID": "notes/05-polynomial-effects.html#residual-plot-another-way-to-spot-nonlinearity",
    "href": "notes/05-polynomial-effects.html#residual-plot-another-way-to-spot-nonlinearity",
    "title": "Polynomial Effects",
    "section": "Residual Plot: Another Way to Spot Nonlinearity",
    "text": "Residual Plot: Another Way to Spot Nonlinearity\nSometimes, the nonlinear relationship is difficult to detect from the scatterplot of Y versus X. Often it helps to fit the linear model and then examine the assumption of linearity in the residuals. It is sometimes easier to detect nonlinearity in the scatterplot of the residuals versus the fitted values.\n\n# Fit linear model\nlm.1 = lm(grad ~ 1 + sat, data = mn)\n\n# Residual plot: Scatterplot\nresidual_plots(lm.1, type = \"s\")\n\n\n\n\nFigure 2: Standardized residuals versus the fitted values for a model regressing six-year graduation rates on median SAT scores. The line \\(Y=0\\) (black) and the loess smoother (blue) are also displayed.\n\n\n\n\nThis plot suggests that the assumption of linearity may be violated; the average residual is not zero at each fitted value. For low fitted values it appears as though the average residual may be less than zero, for moderate fitted values it appears as though the average residual may be more than zero, and for high fitted values it appears as though the average residual may be less than zero.\nNotice that the pattern displayed in the residuals is consistent with the pattern of the observed data in the initial scatterplot (Figure 1). If we look at the data relative to the regression smoother we see that there is not even vertical scatter around this line. At low and high SAT scores the observed data tends to be below the regression line (the regression is over-estimating the average graduation rate), while for moderate SAT scores the observed data tends to be above the regression line (the regression is under-estimating the average graduation rate)."
  },
  {
    "objectID": "notes/05-polynomial-effects.html#are-the-assumptions-more-tenable-for-this-model",
    "href": "notes/05-polynomial-effects.html#are-the-assumptions-more-tenable-for-this-model",
    "title": "Polynomial Effects",
    "section": "Are the Assumptions More Tenable for this Model?",
    "text": "Are the Assumptions More Tenable for this Model?\nMore important than whether the p-value is small, is whether including the quadratic effect improved the assumption violation we noted earlier. To evaluate this, we will examine the two residual plots for the quadratic model: (1) a density plot of the standardized residuals; and (2) a plot of the standardized residuals versus the fitted values.\n\n# Residual plots\nresidual_plots(lm.2)\n\n\n\n\nFigure 3: Residual plots for the quadratic model regressing six-year graduation rate on median SAT scores. LEFT: Density plot of the standardized residuals. The confidence envelope for a normal reference distribution (blue shaded area) is also displayed. RIGHT: Standardized residuals versus the fitted values. The line \\(Y=0\\) (black) and confidence envelope (grey shaded area) for that line are shown, along with the loess smoother (blue) esimating the mean pattern of the residuals.\n\n\n\n\nThe plot of the standardized residuals versus the fitted values suggests that the residuals for the quadratic model are far better behaved; indicating much more consistency with the assumption that the average residual is zero at each fitted value than the linear model. This is the evidence that we would use to justify retaining the quadratic effect in the model. This plot also suggests that the homoskedasticity assumption is tenable. Finally, the empirical density of the residuals also seems consistent with the assumption of normality for this model.\nReminder: The assumption of independence can’t be evaluated from the plots, but is vary important given we rely on it to be able to correctly compute the likelihood. From the design of the study, we haven’t sampled the schools randomly, nor randomly assigned the schools to their levels of the predictor(s), so we can’t infer independence from that. So the question becomes, does knowing the graduation rate of a school in the population give us information about the graduation rate for other schools in the population with the same median SAT score? If the answer is “no”, then we can call the independence assumption tenable. If the answer is “yes”, the independence assumption is violated and we should not use the lm() function or believe that results from that function are valid."
  },
  {
    "objectID": "notes/05-polynomial-effects.html#graphical-interpretation",
    "href": "notes/05-polynomial-effects.html#graphical-interpretation",
    "title": "Polynomial Effects",
    "section": "Graphical Interpretation",
    "text": "Graphical Interpretation\nTo plot the fitted equation, we use the geom_function() layer to add the fitted curve. (Note that we can no longer use geom_abline() since the addition of the polynomial effect implies that a line is no longer suitable.) This layer takes the argument fun= which describes a function that will be plotted as a line. To describe a function, use the syntax function(){...}. Here we use this syntax to describe the function of x (which in the ggplot() global layer is mapped to the sat variable). The fitted equation is then also written in terms of x and placed inside the curly braces. (Note that it is best to be more exact in the coefficient values—don’t round—when you create this plot as even minor differences can grossly change the plot.)\n\n# Scatterplot\nggplot(data = mn, aes(x = sat, y = grad)) +\n    geom_point(alpha = 0.3) +\n  geom_function(\n    fun = function(x) {-366.34 + 62.72*x - 2.15 * x^2},\n    color = \"#0072b2\"\n    ) +\n    theme_light() +\n  xlab(\"Estimated median SAT score (in hundreds)\") +\n  ylab(\"Six-year graduation rate\")\n\n\n\n\nFigure 4: Scatterplot of six-year graduation rate versus median SAT score. The fitted curve from the quadratic regression model (blue line) is also displayed.\n\n\n\n\nThe fitted curve helps us interpret the nature of the relationship between median SAT scores and graduation rates. The effect of median SAT score on graduation rate depends on SAT score (definition of an interaction). For schools with low SAT scores, the effect of SAT score on graduation rate is positive and fairly high. For schools with high SAT scores, the effect of SAT score on graduation rate remains positive, but it has a smaller effect on graduation rates; the effect diminishes."
  },
  {
    "objectID": "notes/05-polynomial-effects.html#algebraic-interpretation",
    "href": "notes/05-polynomial-effects.html#algebraic-interpretation",
    "title": "Polynomial Effects",
    "section": "Algebraic Interpretation",
    "text": "Algebraic Interpretation\nFrom algebra, you may remember that the coefficient in front of the quadratic term (\\(-2.2\\)) informs us of whether the quadratic is an upward-facing U-shape, or a downward-facing U-shape. Since our term is negative, the U-shape is downward-facing. This is consistent with what we saw in the plot. What the algebra fails to show is that, within the range of SAT scores in our data, we only see part of the entire downward U-shape.\nThis coefficient also indicates whether the U-shape is skinny or wide. Although “skinny” and “wide” are only useful as relative comparisons. Algebraically, the comparison is typically to a quadratic coefficient of 1, which is generally not useful in our interpretation. The intercept and coefficient for the linear term help us locate the U-shape in the coordinate plane (moving it right, left, up, or down from the origin). You could work all of this out algebraically.\n\nYou can see how different values of these coefficients affect the curve on Wikipedia. Here is an interactive graph that lets you explore how changing the different coefficients changes the parabola.\n\nWhat can be useful is to find where the minimum (upward facing U-shape) or maximum (downward facing U-shape) occurs. This point is referred to as the vertex of the parabola and can be algebraically determined. To do this we determine the x-location of the vertex by\n\\[\nx_\\mathrm{Vertex} = -\\frac{\\hat{\\beta}_1}{2 \\times \\hat\\beta_2}\n\\]\nwhere, \\(\\hat{\\beta}_1\\) is the estimated coefficient for the linear term and \\(\\hat{\\beta}_2\\) is the estimated coefficient for the quadratic term. The y coordinate for the vertex can then be found by substituting the x-coordinate into the fitted equation. For our example,\n\\[\nx_\\mathrm{Vertex} = -\\frac{62.72}{2 \\times -2.15} = 14.58\n\\]\nand\n\\[\ny_\\mathrm{Vertex} = -366.34 + 62.72(14.58) - 2.15(14.58^2) = 91.08\n\\]\nThis suggests that at a median SAT score of 1458 we predict a six-year graduate rate of 91.08. This x-value also represents the value at which the direction of the effect changes. In our example recall that for higher values of SAT the effect of SAT on graduation rate was diminishing. This is true for schools with median SAT scores up to 1458. For schools with higher SAT scores the effect of SAT score on graduation rate would theoretically be negative, and would be more negative for higher values.\nThis is all theoretical as our data only includes median SAT scores up to 1400. Everything past that value (including the vertex) is extrapolation. Extrapolation is exceedingly sketchy when we start fitting non-linear models. For example, do we really think that the average graduation rate for schools with a median SAT scores higher than 1458 would actually be smaller than for schools at 1458? It is more likely that the effect just flattens out."
  },
  {
    "objectID": "notes/05-polynomial-effects.html#summarizing-the-adopted-interaction-model",
    "href": "notes/05-polynomial-effects.html#summarizing-the-adopted-interaction-model",
    "title": "Polynomial Effects",
    "section": "Summarizing the Adopted Interaction Model",
    "text": "Summarizing the Adopted Interaction Model\nAgain, we will obtain model- and coefficient-level output and use those to summarize the results of the fitted model.\n\n# Model-level output\nglance(lm.4)\n\n\n\n  \n\n\n\nModel 4 explains 90.6% of the variation in graduation rates, an increase of 0.9 percentage points from Model 3. The residual standard error for Model 4, \\(\\hat\\sigma_{\\epsilon}=5.64\\), has also decreased from that for Model 3 indicating that this model has less error than Model 3.\nThe coefficient-level output is:\n\n# Coefficient-level output\ntidy(lm.4)\n\n\n\n  \n\n\n\nWe can use this to obtain the fitted equation:\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}_i} = -&413.80 + 71.65(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i) + \\\\\n&35.07(\\mathrm{Public}_i) - 4.11(\\mathrm{SAT}_i)(\\mathrm{Public}_i)\n\\end{split}\n\\]\nThe interaction tells us that the effect of SAT on graduation rate differs for public and private institutions. Rather than parsing this difference by trying to interpret each of the effects, we will again plot the fitted curves for private and public institutions, and use the plot to aid our interpretation.\nPrivate Institutions\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}_i} &= -413.80 + 71.65(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i) + 35.07(0) - 4.11(\\mathrm{SAT}_i)(0) \\\\\n&= -413.80 + 71.65(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i)\n\\end{split}\n\\]\nPublic Institutions\n\\[\n\\begin{split}\n\\hat{\\mathrm{Graduation~Rate}_i} &= -413.80 + 71.65(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i) + 35.07(1) - 4.11(\\mathrm{SAT}_i)(1) \\\\\n&= -378.73 + 67.54(\\mathrm{SAT}_i) - 2.54(\\mathrm{SAT}^2_i)\n\\end{split}\n\\]\nComparing these two fitted equations, we see that not only is the intercept in the two fitted models different (due to the main effect of sector), but now the coefficient for the interaction between sector and the linear effect of SAT also differs; it is lower for public institutions (by 4.11 units) than for private institutions. The quadratic effect of SAT is the same for both public and private institutions. (If we had adopted the model with both interaction terms, the quadratic effect would also be different across sectors.)\nTo visualize the effect of SAT, we can again plot each fitted curve by including each in a separate geom_function() layer.\n\n# Plot of the fitted model\nggplot(data = mn, aes(x = sat, y = grad)) +\n    geom_point(alpha = 0) +\n    geom_function(\n      fun = function(x) {-378.73 + 67.54 * x - 2.54 * x^2},\n      color = \"#2ec4b6\",\n      linetype = \"dashed\"\n      ) +\n  geom_function(\n    fun = function(x) {-413.80 + 71.65 * x - 2.54 * x^2},\n    color = \"#ff9f1c\",\n    linetype = \"solid\"\n    ) +\n    theme_light() +\n  xlab(\"Estimated median SAT score (in hundreds)\") +\n  ylab(\"Six-year graduation rate\")\n\n\n\n\nFigure 6: Six-year graduation rate as a function of median SAT score for private (orange, solid line) and public (blue, dashed line) institutions.\n\n\n\n\nThe curvature (linear and quadratic slopes) of the lines now looks different for the two sectors because of the interaction term. (Note that interactions with the linear effect of SAT or the quadratic effect of SAT contribute to a change in the curvature.) For both sectors, the effect of median SAT on graduation rates is positive (institutions with higher median SAT scores tend to have higher graduation rates. But, this effect diminishes for institutions with increasingly higher SAT scores. Private schools have higher graduation rates, on average, than public schools for all levels of median SAT score. Moreover, this difference in graduation rates is getting larger at higher levels of SAT (that is the interaction!)."
  },
  {
    "objectID": "readings/05-polynomial-effects.html",
    "href": "readings/05-polynomial-effects.html",
    "title": "📖 Polynomial Effects",
    "section": "",
    "text": "Required\nRefresh your knowledge about parabolas and quadratic functions by going though the Khan Academy Parabolas Intro.\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about polynomial functions. Here are some resources that may be helpful in that endeavor:\n\nVarsity Tutors: Quadratic Function\nMath Centre: Polynomial Function\nOverfitting: A guided tour"
  },
  {
    "objectID": "codebooks/fertility.html",
    "href": "codebooks/fertility.html",
    "title": "fertility.csv",
    "section": "",
    "text": "Human overpopulation is a growing concern and has been associated with depletion of Earth’s natural resources (water is a big one that ) and degredation of the environment. This, in turn, has social and economic consequences such as global tension over resources such as water and food, higher cost of living and higher unemployment rates. The data in fertility.csv were collected from several sources (e.g., World Bank) and are thought to correlate with fertility rates, a measure directly linked to population. The variables are:\n\ncountry: Country name\nregion: Region of the world\nfertility_rate: Average number of children that would be born to a woman if she were to live to the end of her childbearing years and bear children in accordance with age-specific fertility rates.\neduc_female: Average number of years of formal education (schooling) for females\ninfant_mortality: Number of infants dying before reaching one year of age, per 1,000 live births in a given year.\ncontraceptive: Percentage of women who are practicing, or whose sexual partners are practicing, any form of contraception. It is usually measured for women ages 15–49 who are married or in union.\ngni_class: Categorization based on country’s gross national income per capita (calculated using the World Bank Atlas method)\n\nLow: Low-income economies; GNI per capita of $1,025 or less;\nLow/Middle: Lower-middle-income economies; GNI per capita between $1,026 and $3,995;\nUpper/Middle: Upper middle-income economies; GNI per capita between $3,996 and $12,375;\nUpper: High-income economies; GNI per capita of $12,376 or more.\n\nhigh_gni: Dummy variable indicating if the country is has an upper-middle or high income economy (low- or low/middle-income = 0; upper/middle or upper income = 1)\n\n\nPreview\n\n# Import data\nfertility = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/fertility.csv\")\n\n# View data\nfertility\n\n# A tibble: 124 × 7\n   country      region                   ferti…¹ educ_…² infan…³ contr…⁴ gni_c…⁵\n   <chr>        <chr>                      <dbl>   <dbl>   <dbl>   <dbl> <chr>  \n 1 Albania      Europe and Central Asia     1.49     9.1    15        46 Upper/…\n 2 Algeria      Middle East and North A…    2.78     5.9    17.2      57 Upper/…\n 3 Armenia      Europe and Central Asia     1.39    10.8    14.7      57 Upper/…\n 4 Austria      Europe and Central Asia     1.42     8.9     3.3      66 Upper  \n 5 Azerbaijan   Europe and Central Asia     1.92    10.5    30.8      55 Upper/…\n 6 Bahamas, The Latin America and the C…    1.97    11.1    13.9      45 Upper  \n 7 Bangladesh   South Asia                  2.5      4.6    33.1      62 Low/Mi…\n 8 Belgium      Europe and Central Asia     1.65    10.5     3.4      67 Upper  \n 9 Belize       Latin America and the C…    3.08     9.2    15.7      51 Upper/…\n10 Benin        Sub-Saharan Africa          5.13     2      58.5      16 Low    \n# … with 114 more rows, and abbreviated variable names ¹​fertility_rate,\n#   ²​educ_female, ³​infant_mortality, ⁴​contraceptive, ⁵​gni_class\n\n\n\n\nReferences\nRoser, M. (2017). Fertility rate. Our world in data.\nUNICEF. (2016). State of the world’s children 2016. United Nations Population Division’s World Contraceptive Use, household surveys including Demographic and Health Surveys and Multiple Indicator Cluster Surveys.\nWorld Bank (2019). World Bank open data."
  },
  {
    "objectID": "codebooks/wine.html",
    "href": "codebooks/wine.html",
    "title": "wine.csv",
    "section": "",
    "text": "The data in wine.csv includes data on 200 different wines. These data are a subset of a larger database (\\(n = 6,613\\)) from wine.com, one of the biggest e-commerce wine retailers in the U.S. It allows customers to buy wine according to any price range, grape variety, country of origin, etc. The data were made available at http://insightmine.com/. The attributes include:\n\nwine: Wine name\nvintage: Year the wine was produced (centered so that 0 = 2008, 1 = 2009, etc.)\nregion: Region of the world where the wine was produced. These data include seven regions (Australia, California, France, Italy, New Zealand, South Africa, South America).\nvarietal: Grape varietal These data include nine varietals (Cabernet Sauvignon, Chardonnay, Merlot, Pinot Noir, Sauvignon Blanc, Syrah/Shiraz, Zinfandel, Other Red, Other Whites).\nrating: Wine rating on a 100-pt. scale (these are from sources such as Wine Spectator, the Wine Advocate, and the Wine Enthusiast)\nprice: Price in U.S. dollars\n\n\nPreview\n\n# Import data\nwine = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/wine.csv\")\n\n# View data\nwine\n\n# A tibble: 200 × 6\n   wine                                      vintage region varie…¹ rating price\n   <chr>                                       <dbl> <chr>  <chr>    <dbl> <dbl>\n 1 Rust en Vrede Cabernet Sauvignon                4 South… Cabern…     91  30.0\n 2 Bonterra Organically Grown Merlot               4 Calif… Merlot      90  15.0\n 3 Allegrini Palazzo della Torre                   2 Italy  Other …     90  20.0\n 4 Marcarini Barolo Brunate                        2 Italy  Other …     94  56.0\n 5 Chateau Beausejour Duffau                       3 France Other …     96  98.0\n 6 Clos du Marquis                                 2 France Other …     96  60.0\n 7 Altocedro Ano Cero Malbec                       4 South… Other …     92  19.0\n 8 Stag's Leap Wine Cellars Artemis Caberne…       4 Calif… Cabern…     91  33.0\n 9 Duckhorn Three Palms Merlot                     3 Calif… Merlot      95  89  \n10 Migration Russian River Pinot Noir (375M…       4 Calif… Pinot …     93  20.0\n# … with 190 more rows, and abbreviated variable name ¹​varietal"
  },
  {
    "objectID": "codebooks/same-sex-marriage.html",
    "href": "codebooks/same-sex-marriage.html",
    "title": "same-sex-marriage.csv",
    "section": "",
    "text": "The data in same-sex-marriage.csv were collected from the 2008 American National Election Study, conducted jointly by the University of Michigan and Stanford University. These particular data consist of 1,746 American’s responses. The attributes are:\n\nsupport: Dummy-coded variable indicating whether the respondent supports gay marriage? (1=Yes; 0=No)\nattendance: Frequency the respondent attends religious services (0=Never; 1=Few times a year; 2=Once or twice a month; 3=Almost every week; 4=Every week)\ndenomination: Respondent’s religious denomination? (Catholic; Jewish; Protestant; Other)\nfriends: Does the respondent have family or friends that are LGBT? (1=Yes; 0=No)\nage: Respondent’s age, in years\nfemale: Dummy-coded variable indicating whether the respondent is female (1=Yes; 0=No)\n\n\nPreview\n\n# Import data\nsame_sex = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/same-sex-marriage.csv\")\n\n# View data\nsame_sex\n\n# A tibble: 1,746 × 6\n   support attendance denomination friends   age female\n     <dbl>      <dbl> <chr>          <dbl> <dbl>  <dbl>\n 1       0          3 Protestant         0    58      1\n 2       0          4 Protestant         0    39      1\n 3       0          1 Catholic           1    50      0\n 4       0          3 Protestant         0    72      0\n 5       0          0 Other              0    71      1\n 6       0          4 Protestant         1    66      1\n 7       0          4 Protestant         0    56      0\n 8       0          4 Protestant         0    40      1\n 9       0          4 Protestant         0    55      0\n10       0          2 Protestant         1    84      1\n# … with 1,736 more rows"
  },
  {
    "objectID": "codebooks/nhl.html",
    "href": "codebooks/nhl.html",
    "title": "nhl.csv",
    "section": "",
    "text": "Each season, Team Marketing Report (TMR) computes the cost of taking a family of four to a professional sports contest for each of the major sporting leagues. Costs are determined by telephone calls with representatives of the teams, venues and concessionaires. Identical questions were asked in all interviews. Prices for Canadian teams were converted to U.S. dollars and comparison prices were converted using a recent exchange rate.\nThe data in nhl.csv includes data on the cost of attending an NHL game over nine seasons for the current 31 NHL teams. The attributes include:\n\nteam: NHL team name\nfci: Fan cost index (FCI) for each season. There are no data for 2012, since that year the NHL was locked out. The FCI comprises the prices of four (4) average-price tickets, two (2) small draft beers, four (4) small soft drinks, four (4) regular-size hot dogs, parking for one (1) car, two (2) game programs and two (2) least-expensive, adult-size adjustable caps. Costs were determined by telephone calls with representatives of the teams, venues and concessionaires. Identical questions were asked in all interviews.\nyear: NHL season (e.g., 2002 indicates the 2002–2003 NHL season)\nhs_hockey: An dummy coded variable that indicates whether there is state organized high school hockey in the team’s location (0 = no; 1 = yes). This is a proxy for whether there is a hockey tradition in the team’s location.\n\n\nPreview\n\n# Import data\nnhl = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/nhl.csv\")\n\n# View data\nnhl\n\n# A tibble: 279 × 4\n   team              fci  year hs_hockey\n   <chr>           <dbl> <dbl>     <dbl>\n 1 Anaheim Ducks    212.  2002         0\n 2 Anaheim Ducks    229.  2003         0\n 3 Anaheim Ducks    211.  2006         0\n 4 Anaheim Ducks    260.  2007         0\n 5 Anaheim Ducks    274.  2008         0\n 6 Anaheim Ducks    285.  2010         0\n 7 Anaheim Ducks    239.  2011         0\n 8 Anaheim Ducks    285.  2013         0\n 9 Anaheim Ducks    289.  2014         0\n10 Arizona Coyotes  214.  2002         0\n# … with 269 more rows"
  },
  {
    "objectID": "codebooks/mn-schools.html",
    "href": "codebooks/mn-schools.html",
    "title": "mn-schools.csv",
    "section": "",
    "text": "The data in mnSchools.csv were collected from http://www.collegeresults.org and contain 2011 institutional data for \\(n=33\\) Minnesota colleges and universities. The attributes include:\n\nname: College/university name\ngrad: Six-year graduation rate (as a percentage)\npublic: Sector (1 = public college/university, 0 = private college/university)\nsat: Estimated median composite SAT score (in hundreds)\ntuition: Amount of tuition and required fees covering a full academic year for a typical student (in thousands of U.S. dollars)\n\n\nPreview\n\n# Import data\nmn = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/mn-schools.csv\")\n\n# View data\nmn\n\n# A tibble: 33 × 6\n      id name                               grad public   sat tuition\n   <dbl> <chr>                             <dbl>  <dbl> <dbl>   <dbl>\n 1     1 Augsburg College                   65.2      0  10.3    39.3\n 2     3 Bethany Lutheran College           52.6      0  10.6    30.5\n 3     4 Bethel University, Saint Paul, MN  73.3      0  11.4    39.4\n 4     5 Carleton College                   92.6      0  14      54.3\n 5     6 College of Saint Benedict          81.1      0  11.8    43.2\n 6     7 Concordia College at Moorhead      69.4      0  11.4    36.6\n 7     8 Concordia University-Saint Paul    47.9      0   9.9    37.8\n 8     9 Crossroads College                 26.9      0   9.7    25.3\n 9    10 Crown College                      51.3      0  10.3    33.2\n10    11 Gustavus Adolphus College          81.7      0  12.2    43.8\n# … with 23 more rows"
  },
  {
    "objectID": "codebooks/riverview.html",
    "href": "codebooks/riverview.html",
    "title": "riverview.csv",
    "section": "",
    "text": "The data in riverview.csv come from Lewis-Beck & Lewis-Beck (2016) and contain five attributes collected from a random sample of \\(n=32\\) employees working for the city of Riverview, a hypothetical midwestern city. The attributes include:\n\neducation: Years of formal education\nincome: Annual income (in thousands of U.S. dollars)\nseniority: Years of seniority\ngender: Employee’s gender\nmale: Dummy coded gender variable (0 = Female, 1 = Male)\nparty: Political party affiliation\n\n\nPreview\n\n# Import data\ncity = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/riverview.csv\")\n\n# View data\ncity\n\n# A tibble: 32 × 5\n   education income seniority gender     party      \n       <dbl>  <dbl>     <dbl> <chr>      <chr>      \n 1         8   26.4         9 female     Independent\n 2         8   37.4         7 Not female Democrat   \n 3        10   34.2        16 female     Independent\n 4        10   25.5         1 female     Republican \n 5        10   47.0        14 Not female Democrat   \n 6        12   46.5        11 female     Democrat   \n 7        12   52.5        16 female     Independent\n 8        12   37.7        14 Not female Democrat   \n 9        12   50.3        24 Not female Democrat   \n10        14   32.6         5 female     Independent\n# … with 22 more rows\n\n\n\n\n\n\n\n\n\n\nReferences\n\nLewis-Beck, C., & Lewis-Beck, M. (2016). Applied regression: An introduction (2nd ed.). Thousand Oaks, CA: Sage."
  },
  {
    "objectID": "codebooks/mammals.html",
    "href": "codebooks/mammals.html",
    "title": "mammals.csv",
    "section": "",
    "text": "The data in mammals.csv come from Allison & Cicchetti (1976) and contain data on 62 species of mammals. The attributes include:\n\nspecies: Common name of species\nbody_weight: Body weight, in kg\nbrain_weight: Brain weight, in g\nslow_wave: Average slow wave (non-dreaming) sleep per day, in hours\nparadox: Average paradoxical (dreaming) sleep per day, in hours\ntotal_sleep: Average sleep per day, in hours\nlifespan: Average lifespan, in years\ngestation: Average gestation period, in days\npredation: Rating of degree to which species are preyed upon, 1 (low)–5 (high)\nexposure: Rating of sleep exposure from 1 (low exposure; e.g., sleep in a burrow or den)–5 (maximally exposed sleep)\ndanger: Rating of predatory danger based on predation and exposure scales, 1 (low danger)–5 (high degreee of danger)\n\n\nPreview\n\n# Import data\nmammals = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/mammals.csv\")\n\n# View data\nmammals\n\n# A tibble: 62 × 11\n   species       body_…¹ brain…² slow_…³ paradox total…⁴ lifes…⁵ gesta…⁶ preda…⁷\n   <chr>           <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 African elep… 6.65e+3  5712      NA      NA       3.3    38.6     645       3\n 2 African gian… 1   e+0     6.6     6.3     2       8.3     4.5      42       3\n 3 Arctic fox    3.38e+0    44.5    NA      NA      12.5    14        60       1\n 4 Arctic groun… 9.2 e-1     5.7    NA      NA      16.5    NA        25       5\n 5 Asian elepha… 2.55e+3  4603       2.1     1.8     3.9    69       624       3\n 6 Baboon        1.06e+1   180.      9.1     0.7     9.8    27       180       4\n 7 Big brown bat 2.3 e-2     0.3    15.8     3.9    19.7    19        35       1\n 8 Brazilian ta… 1.6 e+2   169       5.2     1       6.2    30.4     392       4\n 9 Cat           3.3 e+0    25.6    10.9     3.6    14.5    28        63       1\n10 Chimpanzee    5.22e+1   440       8.3     1.4     9.7    50       230       1\n# … with 52 more rows, 2 more variables: exposure <dbl>, danger <dbl>, and\n#   abbreviated variable names ¹​body_weight, ²​brain_weight, ³​slow_wave,\n#   ⁴​total_sleep, ⁵​lifespan, ⁶​gestation, ⁷​predation\n\n\n\n\n\n\n\n\n\n\nReferences\n\nAllison, T., & Cicchetti, D. V. (1976). Sleep in mammals: Ecological and constitutional correlates. Science, 194(4266), 732–734. doi:10.1126/science.982039"
  },
  {
    "objectID": "codebooks/graduation.html",
    "href": "codebooks/graduation.html",
    "title": "graduation.csv",
    "section": "",
    "text": "The data in graduation.csv include student-level attributes for \\(n=2,344\\) randomly sampled students who were first-year, full-time students from the 2002 cohort at a large, midwestern research university. Any students who transferred to another institution were removed from the data. The source of these data is Jones-White, Radcliffe, Lorenz, & Soria (2014). The attributes, collected for these students are:\n\nstudent: Student ID number in the dataset\ndegree: Did the student obtain a degree (i.e., graduate) from the institution? (No; Yes)\nact: Student’s ACT score (If the student reported a SAT score, a concordance table was used to transform the score to a comparable ACT score.)\nscholarship: Amount of scholarship offered to student (in thousands of dollars)\nap_courses: Number of Advanced Placement credits at time of enrollment\nfirst_gen: Is the student a first generation college student? (No; Yes)\nnon_traditional: Is the student a non-traditional student (older than 19 years old at the time of enrollment)? (No; Yes)\n\n\nPreview\n\n# Import data\ngraduation = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/graduation.csv\")\n\n# View data\ngraduation\n\n# A tibble: 2,344 × 7\n   student degree   act scholarship ap_courses first_gen non_traditional\n     <dbl> <chr>  <dbl>       <dbl>      <dbl> <chr>     <chr>          \n 1       1 Yes       21         0            0 No        No             \n 2       2 Yes       19         0            0 No        No             \n 3       3 Yes       27         0            0 Yes       No             \n 4       4 Yes       25         0.5          0 Yes       No             \n 5       5 No        28         0           17 Yes       No             \n 6       6 Yes       21         0            0 No        Yes            \n 7       7 Yes       27         0            8 Yes       No             \n 8       8 No        20         0            0 No        No             \n 9       9 Yes       26         0            0 Yes       No             \n10      10 Yes       25         0            4 Yes       No             \n# … with 2,334 more rows\n\n\n\n\n\n\n\n\n\n\nReferences\n\nJones-White, D. R., Radcliffe, P. M., Lorenz, L. M., & Soria, K. M. (2014). Priced out?: The influence of financial aid on the educational trajectories of first-year students starting college at a large research university. Research in Higher Education, 55(4), 329–350."
  },
  {
    "objectID": "codebooks/minneapolis.html",
    "href": "codebooks/minneapolis.html",
    "title": "minneapolis.csv",
    "section": "",
    "text": "The data in minneapolis.csv were provided in Long (2012). They constitute a sample of \\(n=22\\) students taken from a much larger dataset collected by the Minneapolis Public School District. The data were collected to comply with the No Child Left Behind Act of 2001 and began during the 2004-05 school year. The variables included in the data are:\n\nstudent_id: De-identified student ID number\nreading_score: Reading achievement score\\(^\\dagger\\).\ngrade: Grade-level.\nspecial_ed: Is the student recieving special education services?\nattendance: Proportion of attendance based on the number of school days the student attended school during the four year of the study.\n\n\\(^\\dagger\\)The reading achievement scores are based on the reading section of th Northwest Achievement Levels Test (NALT), a multiple-choice, adaptive assessment of students’ academic achievement. The NALT raw scores were converted to vertically equated scaled scores using an IRT model. Higher scale scores indicate more reading achievement.\n\nPreview\n\n# Import data\nmpls = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/minneapolis.csv\")\n\n# View data\nmpls\n\n# A tibble: 88 × 5\n   student_id reading_score grade special_ed attendance\n        <dbl>         <dbl> <dbl> <chr>           <dbl>\n 1          1           172     5 No               0.94\n 2          1           185     6 No               0.94\n 3          1           179     7 No               0.94\n 4          1           194     8 No               0.94\n 5          2           200     5 No               0.91\n 6          2           210     6 No               0.91\n 7          2           209     7 No               0.91\n 8          2           210     8 No               0.91\n 9          3           191     5 No               0.97\n10          3           199     6 No               0.97\n# … with 78 more rows\n\n\n\n\n\n\n\n\n\n\nReferences\n\nLong, J. D. (2012). Longitudinal data analysis for the behavioral sciences using R. Thousand Oaks, CA: Sage Publications, Inc."
  },
  {
    "objectID": "codebooks/vocabulary.html",
    "href": "codebooks/vocabulary.html",
    "title": "vocabulary.csv",
    "section": "",
    "text": "The data in vocabulary.csv, adapted from data provided by Bock (1975), come from the Laboratory School of the University of Chicago and include scaled test scores across four grades from the vocabulary section of the Cooperative Reading Test for \\(n=64\\) students. The attributes in the dataset include:\n\nid: The student ID number\nvocab_08: The scaled vocabulary test score in 8th grade\nvocab_09: The scaled vocabulary test score in 9th grade\nvocab_10: The scaled vocabulary test score in 10th grade\nvocab_11: The scaled vocabulary test score in 11th grade\nfemale: Dummy coded sex variable (0 = Not female, 1 = Female)\n\n\nPreview\n\n# Import data\nvocab = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/vocabulary.csv\")\n\n# View data\nvocab\n\n# A tibble: 64 × 6\n      id vocab_08 vocab_09 vocab_10 vocab_11 female\n   <dbl>    <dbl>    <dbl>    <dbl>    <dbl>  <dbl>\n 1     1     1.75     2.6      3.76     3.68      1\n 2     2     0.9      2.47     2.44     3.43      0\n 3     3     0.8      0.93     0.4      2.27      0\n 4     4     2.42     4.15     4.56     4.21      1\n 5     5    -1.31    -1.31    -0.66    -2.22      0\n 6     6    -1.56     1.67     0.18     2.33      0\n 7     7     1.09     1.5      0.52     2.33      0\n 8     8    -1.92     1.03     0.5      3.04      0\n 9     9    -1.61     0.29     0.73     3.24      0\n10    10     2.47     3.64     2.87     5.38      1\n# … with 54 more rows\n\n\n\n\n\n\n\n\n\n\nReferences\n\nBock, R. D. (1975). Multivariate statistical methods in behavioral research. New York: McGraw-Hill."
  },
  {
    "objectID": "notes/04-optional-likelihood-estimation.html#references",
    "href": "notes/04-optional-likelihood-estimation.html#references",
    "title": "Likelihood: A Framework for Estimation",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "notes/04-optional-likelihood-estimation.html#example-2-using-mle-to-estimate-the-mean-and-standard-deviation",
    "href": "notes/04-optional-likelihood-estimation.html#example-2-using-mle-to-estimate-the-mean-and-standard-deviation",
    "title": "Likelihood: A Framework for Estimation",
    "section": "Example 2: Using MLE to Estimate the Mean and Standard Deviation",
    "text": "Example 2: Using MLE to Estimate the Mean and Standard Deviation\nIn the previous example, we assumed that we knew the value for \\(\\sigma\\). In practice, this often needs to be estimates along with \\(\\mu\\). To do this, you need to set up a search space that includes different combinations of \\(\\mu\\) and \\(\\sigma\\). Here we search \\(\\mu = \\{10.0, 10.1, 10.2,\\ldots, 30.0\\}\\) and \\(\\sigma=\\{0.0, 0.1, 0.2,\\ldots,10.0\\}\\) values from 0.1 to 10.0. (Remember, that \\(\\sigma\\geq0\\)).\nThe crossing() function creates every combination of \\(\\mu\\) and \\(\\sigma\\) that we define in our search space. So, for example, [\\(\\mu=10.0; \\sigma=0.0\\)], ]\\(\\mu=10.0; \\sigma=0.1\\)], [\\(\\mu=10.0; \\sigma=0.2\\)], etc. Since we have included 201 \\(\\mu\\) values and 101 \\(\\sigma\\) values, the search space is \\(201 \\times 101 = 20,301\\) parameter combinations.\n\n# Set up search space\n# Compute the likelihood\nexample_02 = crossing(\n  mu = seq(from = 10, to = 30, by = 0.1),\n  sigma = seq(from = 0, to = 10, by = 0.1)\n  ) |>\n  rowwise() |>\n  mutate(\n    L = prod(dnorm(c(30, 20, 24, 27), mean = mu, sd = sigma))\n    ) |>\n  ungroup()\n\n# Find row with highest likelihood\nexample_02 |>\n  slice_max(L, n = 1)\n\n\n\n  \n\n\n\nThe parameters that maximize the likelihood (in our search space) are a mean of 25.2 and a standard deviation of 3.7. Again, if you need to be more precise in these estimates, you can increase the precision in the by= argument of the seq() functions.\nIn computer science, this method for finding the MLE is referred to as a grid search. This is because the combinations of parameter values in the search space constitute a grid. In the figure below, the search space for each parameter is listed in the first row/column of the table. Every other cell of the table (the “grid”) constitutes a particular combination of the parameters. We are then computing the likelihood for each combination of parameters and searching for the cell with the highest likelihood.\n\n\n\n\n\nFigure 3: Grid showing the combinations of parameter values used in the search space.\n\n\n\n\n\nWhen we have two (or more) parameters we need to estimate the time taken to carry out a grid search is increased in a non-linear way. For example, combining 100 values of each parameter does not result in a search space of 200, but a search space of 10,000. So increasing the precision of both parameters to by=.01 increases each the number of candidates from \\(20,301\\) to \\(2001 \\times 1001 = 2,003,001\\). This increase the computational time it takes to solve the problem.\nIf you are relying on grid search, it is often better to operate with less precision initially, and then identify smaller parts of the grid that can be searched with more precision.\n\n\n\nLikelihood Profile for Multiple Parameters\nWe could also plot the profile of the likelihood for our search space, but this time there would be three dimensions: one dimension for \\(\\mu\\) (x-axis), one dimension for \\(\\sigma\\) (y-axis), and one dimension for the likelihood (z-axis). When we plot the likelihood profile across both \\(\\mu\\) and \\(\\sigma\\), the profile looks like an asymmetrical mountain. The highest likelihood value is at the summit of the mountain and corresponds to \\(\\mu=25.2\\) and \\(\\sigma=3.7\\).\n\n\nCode\n# Load library\nlibrary(plot3D)\n\nscatter3D(x = example_02$mu, y = example_02$sigma, z = example_02$L, \n          pch = 18, cex = 2, theta = 45, phi = 20, ticktype = \"detailed\",\n          xlab = expression(mu), ylab = expression(sigma), zlab = \"Likelihood\",\n          colkey = FALSE,\n          colvar = example_02$L,\n          col = ramp.col(col = c(\"#f6eff7\", \"#bdc9e1\", \"#67a9cf\", \"#1c9099\", \"#016c59\"), n = 100, alpha = 1)\n)\n\n\n\n\n\nFigure 4: Likelihood profile for the search space of both \\(\\mu\\) and \\(\\sigma\\) assuming a normal distribution.\n\n\n\n\nIf we extend our estimation to three or more parameters, we can still use the computational search to find the maximum likelihood estimates (MLEs), but it would be difficult to plot (there would be four or more dimensions). In general, the profile plots are more useful as a pedagogical tool rather than as a way of actually finding the MLEs."
  },
  {
    "objectID": "notes/04-optional-likelihood-estimation.html#likelihood-profile-for-multiple-parameters",
    "href": "notes/04-optional-likelihood-estimation.html#likelihood-profile-for-multiple-parameters",
    "title": "Likelihood: A Framework for Estimation",
    "section": "Likelihood Profile for Multiple Parameters",
    "text": "Likelihood Profile for Multiple Parameters\nWe could also plot the profile of the likelihood for our search space, but this time there would be three dimensions: one dimension for \\(\\mu\\) (x-axis), one dimension for \\(\\sigma\\) (y-axis), and one dimension for the likelihood (z-axis). When we plot the likelihood profile across both \\(\\mu\\) and \\(\\sigma\\), the profile looks like an asymmetrical mountain. The highest likelihood value is at the summit of the mountain and corresponds to \\(\\mu=25.2\\) and \\(\\sigma=3.7\\).\n\n\nCode\n# Load library\nlibrary(plot3D)\n\nscatter3D(x = example_02$mu, y = example_02$sigma, z = example_02$L, \n          pch = 18, cex = 2, theta = 45, phi = 20, ticktype = \"detailed\",\n          xlab = expression(mu), ylab = expression(sigma), zlab = \"Likelihood\",\n          colkey = FALSE,\n          colvar = example_02$L,\n          col = ramp.col(col = c(\"#f6eff7\", \"#bdc9e1\", \"#67a9cf\", \"#1c9099\", \"#016c59\"), n = 100, alpha = 1)\n)\n\n\n\n\n\nFigure 4: Likelihood profile for the search space of both \\(\\mu\\) and \\(\\sigma\\) assuming a normal distribution.\n\n\n\n\nIf we extend our estimation to three or more parameters, we can still use the computational search to find the maximum likelihood estimates (MLEs), but it would be difficult to plot (there would be four or more dimensions). In general, the profile plots are more useful as a pedagogical tool rather than as a way of actually finding the MLEs."
  },
  {
    "objectID": "notes/04-optional-likelihood-estimation.html#complications-with-grid-search",
    "href": "notes/04-optional-likelihood-estimation.html#complications-with-grid-search",
    "title": "Likelihood: A Framework for Estimation",
    "section": "Complications with Grid Search",
    "text": "Complications with Grid Search\nIn practice, there are several issues with the grid search methods we have employed so far. The biggest is that you would not have any idea which values of \\(\\beta_0\\) and \\(\\beta_1\\) to limit the search space to. Essentially you would need to search an infinite number of values unless you could limit the search space in some way. For many common methods (e.g., linear regression) finding the ML estimates is mathematically pretty easy (if we know calculus; see the section Using Calculus to Determine the MLEs). For more complex methods (e.g., mixed-effect models) there is not a mathematical solution. Instead, mathematics is used to help limit the search space and then a grid search is used to hone in on the estimates.\nAlthough not a complication, we made an assumption about the value of the residual standard error, that it was equivalent to sigma(errors). In practice, this value would also need to be estimated, along with the coefficients."
  },
  {
    "objectID": "notes/04-optional-likelihood-estimation.html#estimating-the-residual-variation-maximum-likelihood-vs.-ordinary-least-squares",
    "href": "notes/04-optional-likelihood-estimation.html#estimating-the-residual-variation-maximum-likelihood-vs.-ordinary-least-squares",
    "title": "Likelihood: A Framework for Estimation",
    "section": "Estimating the Residual Variation: Maximum Likelihood vs. Ordinary Least Squares",
    "text": "Estimating the Residual Variation: Maximum Likelihood vs. Ordinary Least Squares\nThe estimates of the residual standard error differ because the two estimation methods use different criteria to optimize over; OLS estimation finds the estimates that minimize the sum of squared errors, and ML finds the estimates that maximize the likelihood. Because of the differences, it is important to report how the model was estimated in any publication.\nBoth estimation methods have been well studied, and the resulting residual standard error from these estimation methods can be computed directly once we have the coefficient estimates (which are the same for both methods). Namely, the residual standard error resulting from OLS estimation is:\n\\[\n\\hat\\sigma_{\\epsilon}= \\sqrt{\\frac{\\left(Y_i - \\hat{Y}_i\\right)^2}{n-p-1}}\n\\]\nwhere p is the number of predictors in the model. And the residual standard error resulting from ML estimation is:\n\\[\n\\hat\\sigma_{\\epsilon}=\\sqrt{\\frac{\\left(Y_i - \\hat{Y}_i\\right)^2}{n}},\n\\]\nThe smaller denominator from the OLS estimate produces a higher overall estimate of the residual variation (more uncertainty). When n is large, the differences between the OLS and ML estimates of the residual standard error are minimal and can safely be ignored. When n is small, however, these differences can impact statistical results. For example, since the residual standard error is used to compute the standard error estimates for the coefficients, the choice of ML or OLS will have an effect on the size of the t- and p-values for the coefficients. (In practice, it is rare to see the different estimation methods producing substantively different findings, especially when fitting general linear models.)\nLastly, we note that the value of log-likelihood is the same for both the ML and OLS estimated models. The result from the ML output was:\n\\[\n\\begin{split}\n-2 \\ln(\\mathrm{Likelihood}) &= 78.91 \\\\[1ex]\n\\ln(\\mathrm{Likelihood}) &= -39.45\n\\end{split}\n\\]\nThe log-likelihood for the OLS estimated model is:\n\n# Log-likelihood for OLS model\nlogLik(lm(y ~ 1 + x))\n\n'log Lik.' -39.45442 (df=3)\n\n\nThis is a very useful result. It allows us to use lm() to estimate the coefficients from a model and then use its log-likelihood value in the same way as if we had fitted the model using ML. This will be helpful when we compute measure such as information criteria later in the course.\n\nIn many applications of estimation, it is useful to use a criterion which is modified variant of the likelihood. This variant omits “nuisance parameters” (parameters which are not of direct interest and subsequently not needed in the estimation method) from the computation of the likelihood. This restricted version of the likelihood is then maximized and the estimation method using this modified likelihood is called Restricted Maximum Likelihood (REML).\nWhen REML is used to estimate parameters, the residual standard error turns out to be the same as that computed in the OLS estimation. As such, sometimes this estimate is referred to as the REML estimate of the residual standard error."
  },
  {
    "objectID": "notes/06-information-criteria-and-model-selection.html",
    "href": "notes/06-information-criteria-and-model-selection.html",
    "title": "Information Criteria and Model Selection",
    "section": "",
    "text": "Preparation\nIn this set of notes, you will use information theoretic approaches (e.g., information criteria) to select one (or more) empirically supported model from a set of candidate models. To do so, we will use the mn-schools.csv dataset:\n\nCSV File\nData Codebook\n\nOur goal will be to examine if (and how) academic “quality” of the student-body (measured by SAT score) is related to institutional graduation rate.\n\n# Load libraries\nlibrary(AICcmodavg)\nlibrary(broom)\nlibrary(tidyverse)\n\n# Read in data\nmn = read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/mn-schools.csv\")\n\n\n\n\nWorking Hypotheses and Candidate Models\nOver the prior sets of notes, we have considered several models that explain variation in graduation rates.\n\\[\n\\begin{split}\n\\mathbf{Model~1:~} \\quad \\hat{\\mathrm{Graduation~Rate}}_i &= \\beta_0 + \\beta_1(\\mathrm{SAT}_i) + \\epsilon_i \\\\[1ex]\n\\mathbf{Model~2:~} \\quad \\hat{\\mathrm{Graduation~Rate}}_i &= \\beta_0 + \\beta_1(\\mathrm{SAT}_i) + \\beta_2(\\mathrm{Public}_i) + \\epsilon_i \\\\[1ex]\n\\mathbf{Model~3:~} \\quad \\hat{\\mathrm{Graduation~Rate}}_i &= \\beta_0 + \\beta_1(\\mathrm{SAT}_i) + \\beta_2(\\mathrm{Public}_i) + \\beta_3(\\mathrm{SAT}_i)(\\mathrm{Public}_i) + \\epsilon_i\n\\end{split}\n\\]\nwhere all three models have \\(\\epsilon_i\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\).\n\n# Fit candidate models\nlm.1 = lm(grad ~ 1 + sat, data = mn)\nlm.2 = lm(grad ~ 1 + sat + public, data = mn)\nlm.3 = lm(grad ~ 1 + sat + public + sat:public, data = mn)\n\nEach of these models correspond to a different scientific working hypothesis about how graduation rates and median SAT scores are related:\n\nH1: The effect of median SAT scores on graduation rates is constant across all levels of SAT, and educational sector does not have an effect on graduation rates.\nH2: The effect of median SAT scores on graduation rates is constant across all levels of SAT. And, while there is also an effect of educational sector on graduation rates, the effect of median SAT scores is identical for both public and private schools.\nH3: The effect of median SAT scores on graduation rates is constant across all levels of SAT. Moreover, this effect varies across educational sector.\n\n\nThese working hypotheses are typically created from the theory and previous empirical work in a substantive area. They need to be translated into a statistical models, which can be quite difficult, especially if there is not a lot of theory to guide this translation.\n\nOne method for choosing among a set of candidate models is to pick based on the residuals. In this method, we adopt the model for which the data best meet the assumptions, and employing the principle of parsimony—adopting the more parsimonious model when there are multiple models that produce equally “good” residuals. Additionally, we can use the likelihood ratio test (LRT) to help make this decision. This method provides additional empirical evidence about which model is supported when the candidate models are nested.\nUnfortunately, we cannot always use the LRT to select models, since there are many times when the set of candidate models are not all nested. Information criteria give us metrics to compare the empirical support for models whether they are nested or not. In the remainder of these notes, we will examine several of the more popular information criteria metrics used for model selection.\n\n\n\nAkiake’s Information Criteria (AIC)\nThe AIC metric is calculated by adding a penalty term to the model’s deviance (\\(-2\\ln(\\mathrm{Likelihood})\\)).\n\\[\n\\begin{split}\nAIC &= \\mathrm{Deviance} + 2k \\\\[1ex]\n&= -2\\ln(\\mathcal{L}) + 2k\n\\end{split}\n\\]\nwhere k is the number of parameters (including the residual standard error) being estimated in the model. (Recall that the value for k is given as df in the logLik() output.)\nRemember that deviance is similar to error (it measures model-data misfit), and so models that have lower values of deviance are more empirically supported. The problem with deviance, however, is that more complex models tend to have smaller deviances and are, therefore, more supported than their simpler counterparts. Unfortunately, these complex models tend not to generalize as well as simpler models (this is called overfitting). The AIC metric’s penalty term penalizes the deviance more when a more complex model is fitted; it is offsetting the lower degree of model-data misfit in complex models by increasing the ‘misfit’ based on the number of parameters.\nThis penalty-adjusted measure of ‘misfit’ is called Akiake’s Information Criteria (AIC). We compute the AIC for each of the candidate models and the model with the lowest AIC is selected. This model, we say, is the candidate model with the most empirical support. Below we compute the AIC for the first candidate model.\n\n# Compute AIC for Model 1\n# logLik(lm.1)\n-2*-113.5472 + 2*3\n\n[1] 233.0944\n\n\nWe could also use the AIC() function to compute the AIC value directly. Here we compute the AIC for each of the candidate models.\n\n# Model 1\nAIC(lm.1)\n\n[1] 233.0944\n\n# Model 2\nAIC(lm.2)\n\n[1] 225.5929\n\n# Model 3\nAIC(lm.3)\n\n[1] 227.0044\n\n\nBased on the AIC values, the candidate model with the most empirical evidence is the model with main effects of median SAT score and educational sector (Model 2); it has the lowest AIC.\nLastly, we note that the AIC value is produced as a column in the model-level output from the glance() function. (Note that the df column from glance() does NOT give the number of model parameters.)\n\n# Model-level output for H2\nglance(lm.2)\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences"
  },
  {
    "objectID": "notes/06-information-criteria-and-model-selection.html#empirical-support-is-for-the-working-hypotheses",
    "href": "notes/06-information-criteria-and-model-selection.html#empirical-support-is-for-the-working-hypotheses",
    "title": "Information Criteria and Model Selection",
    "section": "Empirical Support is for the Working Hypotheses",
    "text": "Empirical Support is for the Working Hypotheses\nBecause the models are proxies for the scientific working hypotheses, the AIC ends up being a measure of empirical support for any particular working hypothesis. Using the AIC, we can rank order the models (and subsequently the working hypotheses) based on their level of empirical support. Ranked in order of empirical support, the three scientific working hypotheses are:\n\nH2: The effect of median SAT scores on graduation rates is constant across all levels of SAT. And, while there is also an effect of educational sector on graduation rates, the effect of median SAT scores is identical for both public and private schools.\nH3: The effect of median SAT scores on graduation rates is constant across all levels of SAT. Moreover, this effect varies across educational sector.\nH1: The effect of median SAT scores on graduation rates is constant across all levels of SAT, and educational sector does not have an effect on graduation rates.\n\nIt is important to remember that the phrase given the data and other candidate models is highly important. The ranking of models/working hypotheses is a relative ranking of the models’ level of empirical support contingent on the candidate models we included in the comparison and the data we used to compute the AIC.\nAs such, this method is not able to rank any hypotheses that you did not consider as part of the candidate set of scientific working hypotheses. Moreover, the AIC is a direct function of the likelihood which is based on the actual model fitted as a proxy for the scientific working hypothesis. If the predictors used in any of the models had been different, it would lead to different likelihood and AIC values, and potentially a different rank ordering of the hypotheses.\nThe ranking of models is also based on the data we have. Even though Model 2 may not be what we expect substantively, it is more empirically supported than the interaction model (or the simple model). Why? Well, we only have data in a certain range of median SAT scores for public and private schools. Within this range of scores, Model 2 is more empirically supported, even after accounting for the additional complexity of the interaction model. If we had a broader range of median SAT scores in these two sectors (or had differentiated between private for-profit and private not-for-proft school), that evidence might support a different model. This is very important. Model 2 is the most empirically supported candidate model GIVEN the three candidate models we compared and the data we used to compute the AIC metric.\n\nThe model selection and ranking is contingent on both the set of candidate models you are evaluating, and the data you have.\n\nBased on the AIC values for the three candidate models we ranked the hypotheses based on the amount of empirical support:\n\n\nCode\ncand_models = data.frame(\n  Model = c(\n    \"Model 2\",\n    \"Model 3\",\n    \"Model 1\"\n    ),\n  k = c(4, 5, 3),\n  AIC = c(AIC(lm.2), AIC(lm.3), AIC(lm.1))\n) \n\ncand_models |>\n  gt() |>\n  cols_align(\n    columns = c(Model),\n    align = \"left\"\n  ) |>\n  cols_align(\n    columns = c(k, AIC),\n    align = \"center\"\n  ) |>\n  cols_label(\n    Model = md(\"*Model*\"),\n    k = md(\"*k*\"),\n    AIC = md(\"*AIC*\")\n  ) |>\n  tab_options(\n    table.width = pct(40)\n  )\n\n\n\n\n\n\nTable 1:  Models rank-ordered by the amount of empirical support as measured by the AIC. \n  \n  \n    \n      Model\n      k\n      AIC\n    \n  \n  \n    Model 2\n4\n225.5929\n    Model 3\n5\n227.0044\n    Model 1\n3\n233.0944"
  },
  {
    "objectID": "notes/06-information-criteria-and-model-selection.html#pretty-printing-tables-of-model-evidence-for-quarto-documents",
    "href": "notes/06-information-criteria-and-model-selection.html#pretty-printing-tables-of-model-evidence-for-quarto-documents",
    "title": "Information Criteria and Model Selection",
    "section": "Pretty Printing Tables of Model Evidence for Quarto Documents",
    "text": "Pretty Printing Tables of Model Evidence for Quarto Documents\nWe can format the output from aictab() to be used in the gt() function. Because there are multiple classes associated with the output from the aictab() function, we first pipe model_evidence object into the data.frame() function. Viewing this, we see that the data frame, also includes an additional column that gives the relative likelihoods (ModelLik).\n\n# Create data frame to format into table\ntab_01 = model_evidence %>%\n  data.frame()\n\n# View table\ntab_01\n\n\n\n  \n\n\n\nThen we can use the select() function to drop the LL and Cum.Wt columns from the data frame. The log-likelihood is redundant to the information in the AICc column, since AICc is a function of log-likelihood and the other information in the table. The cumulative weight can also easily be computed from the information in the AICcWt column.\n\n# Drop columns\ntab_01 = tab_01 |>\n  select(-LL, -Cum.Wt)\n\n# View table\ntab_01\n\n\n\n  \n\n\n\nWe can then pipe the tab_01 data frame into the gt() function to format the table for pretty-printing in Quarto. For some column labels, I use the html() function in order to use HTML symbols to create the Greek letter Delta and the scripted “L”.\n\n# Create knitted table\ntab_01 |>\n  gt() |>\n  cols_align(\n    columns = c(Modnames),\n    align = \"left\"\n  ) |>\n  cols_align(\n    columns = c(K, AICc, Delta_AICc, ModelLik, AICcWt),\n    align = \"center\"\n  ) |>\n  cols_label(\n    Modnames = md(\"*Model*\"),\n    K = md(\"*k*\"),\n    AICc = md(\"*AICc*\"),\n    Delta_AICc = html(\"&#916;AICc\"),\n    ModelLik = html(\"Rel(&#8466;)\"),\n    AICcWt = md(\"*AICc Wt.*\")\n  ) |>\n  tab_options(\n    table.width = pct(50)\n  ) |>\n  tab_footnote(\n    footnote = html(\"Rel(&#8466;) = Relative likelihood\"),\n    locations = cells_column_labels(columns = ModelLik)\n  )\n\n\n\n\n\nTable 7:  Models rank-ordered by the amount of empirical support as measured by the AICc after removing Model 1 from the candidate set. Other evidence includes the ΔAICc, relative likelihood, and model probability (AICc weight) for each model. \n  \n  \n    \n      Model\n      k\n      AICc\n      ΔAICc\n      Rel(ℒ)1\n      AICc Wt.\n    \n  \n  \n    Model 2\n4\n227.0215\n0.000000\n1.00000000\n0.73327032\n    Model 3\n5\n229.2266\n2.205140\n0.33201673\n0.24345802\n    Model 1\n3\n233.9220\n6.900556\n0.03173681\n0.02327166\n  \n  \n  \n    \n      1 Rel(ℒ) = Relative likelihood"
  },
  {
    "objectID": "readings/06-information-criteria-and-model-selection.html",
    "href": "readings/06-information-criteria-and-model-selection.html",
    "title": "📖 Polynomial Effects",
    "section": "",
    "text": "Required\nRead the following:\n\nElliott, L. P., & Brook, B. W. (2007). Revisiting Chamberlin: Multiple working hypotheses for the 21st century. BioScience, 57(7), 608–614. doi: 10.1641/B570708\n\n\nAdditional Resources\nIn addition to the notes and what we cover in class, there are many other resources for learning about information criteria and model selection. Here are some resources that may be helpful in that endeavor:\n\nAnderson, D. R. (2008). Model based inference in the life sciences: A primer on evidence. New York: Springer. [Optional Textbook]\nBurnham, K. P., & Anderson, D. R. (2002). Model selection and multimodel inference: A practical information-theoretic approach. New York: Springer.\nBurnham, K. P., Anderson, D. R., & Huyvaert, K. P. (2010). AIC model selection and multimodel inference in behavioral ecology: Some background, observations, and comparisons. Behavioral Ecology and Sociobiology, 65(1), 23–35."
  },
  {
    "objectID": "codebooks/carbon.html",
    "href": "codebooks/carbon.html",
    "title": "carbon.csv",
    "section": "",
    "text": "Carbon dioxide emissions are the primary driver of global climate change. Some of the biggest predictors of these emissions include economic growth, industrialization, and urbanization. The data in carbon.csv are from 2017 and include some predictors of carbon dioxide emissions for several countries around the world. The variables are:\n\ncountry: Country name\nregion: Region of the world\nco2: Carbon dioxide measure from the burning of fossil fuels (metric tons per person)\nwealth: A measure of a country’s wealth based on its GDP; higher values indicate wealthier countries\nurbanization: Annual urban population growth (as a percentage)\n\n\nPreview\n\n# Import data\ncarbon = readr::read_csv(file = \"https://raw.githubusercontent.com/zief0002/bespectacled-antelope/main/data/carbon.csv\")\n\n# View data\ncarbon\n\n# A tibble: 189 × 5\n   country             region      co2 wealth urbanization\n   <chr>               <chr>     <dbl>  <dbl>        <dbl>\n 1 Afghanistan         Asia      0.254   1.07        3.35 \n 2 Albania             Europe    1.59    3.75        1.32 \n 3 Algeria             Africa    3.69    3.54        2.81 \n 4 Angola              Africa    1.12    2.71        4.31 \n 5 Antigua and Barbuda Americas  5.88    4.36        0.432\n 6 Argentina           Americas  4.41    4.49        1.15 \n 7 Armenia             Europe    1.89    3.72        0.309\n 8 Australia           Oceania  16.9     5.60        1.66 \n 9 Austria             Europe    7.75    5.82        0.836\n10 Azerbaijan          Europe    3.7     3.79        1.47 \n# … with 179 more rows\n\n\n\n\nReferences\nWorld Bank (2019). World Bank open data."
  }
]